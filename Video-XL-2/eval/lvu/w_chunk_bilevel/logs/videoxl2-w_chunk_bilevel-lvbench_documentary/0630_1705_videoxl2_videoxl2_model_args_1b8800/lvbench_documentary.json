{
    "args": {
        "config": "",
        "model": "videoxl2",
        "tasks": "lvbench_documentary",
        "model_args": "pretrained=/share/minghao/Models2/Video-XL-2,conv_template=qwen_1_5,model_name=llava_qwen,max_frames_num=600,fps=1,max_fps=4,block_size_chosed=4,prev_blocks_num=11,video_decode_backend=decord,selected_info_file_path=./selected_infos/lvbench/lvbench_documentary.json,attn_implementation=sdpa,",
        "num_fewshot": null,
        "batch_size": "1",
        "max_batch_size": null,
        "device": null,
        "output_path": "/share/minghao/VideoProjects/Upload/Video-XL/Video-XL-2/eval/lvu/w_chunk_bilevel/logs/videoxl2-w_chunk_bilevel-lvbench/0630_1705_videoxl2_videoxl2_model_args_1b8800",
        "limit": null,
        "use_cache": null,
        "cache_requests": null,
        "check_integrity": false,
        "write_out": false,
        "log_samples": true,
        "wandb_log_samples": false,
        "log_samples_suffix": "videoxl2",
        "system_instruction": null,
        "apply_chat_template": false,
        "fewshot_as_multiturn": false,
        "show_config": false,
        "include_path": null,
        "gen_kwargs": "",
        "verbosity": "INFO",
        "wandb_args": "",
        "timezone": "Asia/Singapore",
        "hf_hub_log_args": ",output_path=./logs/videoxl2-w_chunk_bilevel-lvbench,token=YOUR_HF_KEY",
        "predict_only": false,
        "seed": [
            0,
            1234,
            1234,
            1234
        ],
        "trust_remote_code": false
    },
    "model_configs": {
        "task": "lvbench_documentary",
        "dataset_path": "/share/minghao/Datasets/LVBench",
        "dataset_name": "lvbench_documentary",
        "dataset_kwargs": {
            "token": true
        },
        "test_split": "train",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function lvbench_mc_doc_to_visual at 0x7f4babb20430>",
        "doc_to_text": "<function lvbench_mc_doc_to_text at 0x7f4b499d6c20>",
        "doc_to_target": "answer",
        "process_results": "<function lvbench_mc_process_results at 0x7f4b499d72e0>",
        "description": "",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
            {
                "metric": "lvbench_mc_accuracy",
                "aggregation": "<function lvbench_mc_aggregate_results at 0x7f4b499d7910>",
                "higher_is_better": true
            }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
            "max_new_tokens": 16,
            "temperature": 0.0,
            "top_p": 1.0,
            "num_beams": 1,
            "do_sample": false,
            "until": [
                "\n\n"
            ]
        },
        "repeats": 1,
        "should_decontaminate": false,
        "lmms_eval_specific_kwargs": {
            "default": {
                "sub_task": "lvbench_documentary",
                "post_prompt": "Answer with the option's letter from the given choices directly."
            },
            "sub_task": "lvbench_documentary",
            "post_prompt": "Answer with the option's letter from the given choices directly."
        }
    },
    "logs": [
        {
            "doc_id": 0,
            "doc": {
                "candidates": [
                    "(A) A skull and bones",
                    "(B) A cat",
                    "(C) A word",
                    "(D) A Dog"
                ],
                "answer": "(A) A skull and bones",
                "time_reference": "06:23-06:23",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "There is a scene of of cutting trees to get the sap in the video. Immediately after it there is a scene of man's arm. What's tattooed on the man's arm?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(A) A skull and bones",
            "arguments": [
                "There is a scene of of cutting trees to get the sap in the video. Immediately after it there is a scene of man's arm. What's tattooed on the man's arm?\nA. (A) A skull and bones\nB. (B) A cat\nC. (C) A word\nD. (D) A Dog\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd10f0>"
                    ]
                },
                0,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a0cea840a7bdf73ba9c62ef7c17d4d0f1d30805eae5fe6fa590c3185e14a08e4",
            "target_hash": "71565cd097ddeee5367d1f9cbf7dc1167fc690b67ee63459a425a871c16d4bee",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 1,
            "doc": {
                "candidates": [
                    "(A) Noon",
                    "(B) Night",
                    "(C) Early morning",
                    "(D) Morning"
                ],
                "answer": "(B) Night",
                "time_reference": "08:45-09:00",
                "question_type": [
                    "reasoning"
                ],
                "question": "When do animals move out?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(B) Night",
            "arguments": [
                "When do animals move out?\nA. (A) Noon\nB. (B) Night\nC. (C) Early morning\nD. (D) Morning\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd12a0>"
                    ]
                },
                1,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e7bf3c71225d9fce41b0984e66ceb8552e1935db5ae021053a236447ebca0325",
            "target_hash": "6839078ff44c8cce6160dbd5832eea2ecaae54e74742689943e01460ba17b62b",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 2,
            "doc": {
                "candidates": [
                    "(A) Fish",
                    "(B) Lamb",
                    "(C) Deer",
                    "(D) Wild boar"
                ],
                "answer": "(D) Wild boar",
                "time_reference": "13:30-13:39",
                "question_type": [
                    "event understanding",
                    "temporal grounding",
                    "reasoning"
                ],
                "question": "What is the most likely animal the men try to hunt at 13:13?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(D) Wild boar",
            "arguments": [
                "What is the most likely animal the men try to hunt at 13:13?\nA. (A) Fish\nB. (B) Lamb\nC. (C) Deer\nD. (D) Wild boar\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1de0>"
                    ]
                },
                2,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "610b7f84fbdf321dd59e82dd1e2c5fef28606056a6305773d219b4c7f4ed154f",
            "target_hash": "0b172bde3525212da94ee0ce3c7931035571db8d7155b48357444a65acf0537a",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 3,
            "doc": {
                "candidates": [
                    "(A) Marine",
                    "(B) Plateau",
                    "(C) Rain forest",
                    "(D) Forest"
                ],
                "answer": "(C) Rain forest",
                "time_reference": "07:00-17:00",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "The video contains lots of ___ clips.",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(C) Rain forest",
            "arguments": [
                "The video contains lots of ___ clips.\nA. (A) Marine\nB. (B) Plateau\nC. (C) Rain forest\nD. (D) Forest\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2d70>"
                    ]
                },
                3,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d61ec87c8176ffce0e26aec9a401f33b0fc6b6a17b00aaff0575bdd22daa81a9",
            "target_hash": "229248ff3c9e7d4221c51119d4a1781e95711234cbcab4c9bb9b7326e90560f0",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 4,
            "doc": {
                "candidates": [
                    "(A) Chopsticks",
                    "(B) Spoon",
                    "(C) Spatulas",
                    "(D) Syringes"
                ],
                "answer": "(A) Chopsticks",
                "time_reference": "19:02-19:03",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What tool do people use to pick up a sticky, white to transparent food substance?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(A) Chopsticks",
            "arguments": [
                "What tool do people use to pick up a sticky, white to transparent food substance?\nA. (A) Chopsticks\nB. (B) Spoon\nC. (C) Spatulas\nD. (D) Syringes\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3c70>"
                    ]
                },
                4,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "fe2c581ee49a9ed98440808cd1b0748a8af21894eac837c562dae07b0f5d6571",
            "target_hash": "593ae058373e510391b060415c31a734ef04d8753df4999631410f8f413bb9d4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 5,
            "doc": {
                "candidates": [
                    "(A) Swimming",
                    "(B) Rowing",
                    "(C) Fishing",
                    "(D) Diving"
                ],
                "answer": "(C) Fishing",
                "time_reference": "16:30-17:15",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "question": "What are the people going to do from 16:30-16:35?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(C) Fishing",
            "arguments": [
                "What are the people going to do from 16:30-16:35?\nA. (A) Swimming\nB. (B) Rowing\nC. (C) Fishing\nD. (D) Diving\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044580>"
                    ]
                },
                5,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "7c2323c3b1582a985a34ff0ea81643b58c16f06e2b0fbd2e53237b14d292acf4",
            "target_hash": "269befd351d45641797efb67d1aec89d67e4f3b69c5dfce33d48e6420e860f81",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 6,
            "doc": {
                "candidates": [
                    "(A) Car",
                    "(B) Bike",
                    "(C) Motocycle",
                    "(D) Skateboard"
                ],
                "answer": "(C) Motocycle",
                "time_reference": "31:02-32:00",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What kind of transportation do the people carry on the boat?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(C) Motocycle",
            "arguments": [
                "What kind of transportation do the people carry on the boat?\nA. (A) Car\nB. (B) Bike\nC. (C) Motocycle\nD. (D) Skateboard\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045ab0>"
                    ]
                },
                6,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "38b30d3d8127ab54e9faf736529851da5e4fd750c2369b63480bbec6129eb2a1",
            "target_hash": "a51ebbe2cdadfa17707070e2141b53781568df1368e6a19085fd7838631bc13c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 7,
            "doc": {
                "candidates": [
                    "(A) Reporter spends three months with the Penan people, understanding their psychological characteristics and documenting the lifestyle of some Penan people living in the city",
                    "(B) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also documented the lifestyle of some Penan people living in the city",
                    "(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village",
                    "(D) Reporter spends three months with the Penan people, understanding their psychological characteristics and recording the lifestyle of some of the Penan people living in the countryside"
                ],
                "answer": "(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village",
                "time_reference": "22:17-42:05",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "question": "What happens between 22:17-42:05?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village",
            "arguments": [
                "What happens between 22:17-42:05?\nA. (A) Reporter spends three months with the Penan people, understanding their psychological characteristics and documenting the lifestyle of some Penan people living in the city\nB. (B) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also documented the lifestyle of some Penan people living in the city\nC. (C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village\nD. (D) Reporter spends three months with the Penan people, understanding their psychological characteristics and recording the lifestyle of some of the Penan people living in the countryside\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044370>"
                    ]
                },
                7,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4a72c5369f823027b8a0c0fb96950f285116dcd252ac7c7fbcdd8542defab4fc",
            "target_hash": "72ab04df2816ae9b6693073cb4aa2bc467ad61ac1a0d211a92143529a3169393",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 8,
            "doc": {
                "candidates": [
                    "(A) Voldka",
                    "(B) Sparkling water",
                    "(C) Spirte",
                    "(D) Rice wine"
                ],
                "answer": "(D) Rice wine",
                "time_reference": "32:40-33:00",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "question": "One of the woman give the reporter a kind of beverage in a glass cup, that is ___.",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(D) Rice wine",
            "arguments": [
                "One of the woman give the reporter a kind of beverage in a glass cup, that is ___.\nA. (A) Voldka\nB. (B) Sparkling water\nC. (C) Spirte\nD. (D) Rice wine\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd0d90>"
                    ]
                },
                8,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3c1a2b7519ad8db7e3e47384b663e332c1865ee38301835cbe81af1bc9411fb5",
            "target_hash": "276e9d0a21ae717991e61bd49b3cfd002b551d81283320ee06c5d5fedc375070",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 9,
            "doc": {
                "candidates": [
                    "(A) Playing with toys",
                    "(B) Writing homework",
                    "(C) Scratching her leg",
                    "(D) Playing with monkey"
                ],
                "answer": "(D) Playing with monkey",
                "time_reference": "34:22-34:22",
                "question_type": [
                    "entity recognition",
                    "temporal grounding",
                    "event understanding"
                ],
                "question": "What is Saya's daughter doing at 34:22?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(D) Playing with monkey",
            "arguments": [
                "What is Saya's daughter doing at 34:22?\nA. (A) Playing with toys\nB. (B) Writing homework\nC. (C) Scratching her leg\nD. (D) Playing with monkey\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd29b0>"
                    ]
                },
                9,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "06b22ed0888bd7df4be15d3bb4f399b4f513ba0710d785c88349bed9b399c1d4",
            "target_hash": "452c267539ddaae2e3889c08ef81b4dea37c11be1a402537c9f32eaf898c5272",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 10,
            "doc": {
                "candidates": [
                    "(A) Making a fire",
                    "(B) Peeling a tree",
                    "(C) Sharpening a knife",
                    "(D) Making a food"
                ],
                "answer": "(A) Making a fire",
                "time_reference": "38:00-39:00",
                "question_type": [
                    "event understanding"
                ],
                "question": "There is a close-up scene of a woman doing something on a leaf. She is ___.",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(A) Making a fire",
            "arguments": [
                "There is a close-up scene of a woman doing something on a leaf. She is ___.\nA. (A) Making a fire\nB. (B) Peeling a tree\nC. (C) Sharpening a knife\nD. (D) Making a food\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3ca0>"
                    ]
                },
                10,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "61a4466eefd986542b6fb6cbd85129022b73588f55ffb699c3f88f5c3a5a85be",
            "target_hash": "61ca6c74f5f4250a161c0e17aa5c3e338de2bd4bc2c95802f341b587aec9d7c9",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 11,
            "doc": {
                "candidates": [
                    "(A) Mantis",
                    "(B) Scorpion",
                    "(C) Spider",
                    "(D) Cockroach"
                ],
                "answer": "(B) Scorpion",
                "time_reference": "39:49-39:49",
                "question_type": [
                    "entity recognition"
                ],
                "question": "___ is an insect appears in the video.",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(B) Scorpion",
            "arguments": [
                "___ is an insect appears in the video.\nA. (A) Mantis\nB. (B) Scorpion\nC. (C) Spider\nD. (D) Cockroach\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2fe0>"
                    ]
                },
                11,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "713c9b56e6c31a749df4c57f6ac3eff7d673b2134f55127f549d7bdd55281e92",
            "target_hash": "024b60c62fe1a4f6c8aef3bc2b4ad5def4f717b31d820436202550e439d34c4b",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 12,
            "doc": {
                "candidates": [
                    "(A) Boat",
                    "(B) Bike",
                    "(C) Motorcycle",
                    "(D) Car"
                ],
                "answer": "(D) Car",
                "time_reference": "39:00-41:00",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "What kind of transportation does the reporter use to leave the tribe?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(D) Car",
            "arguments": [
                "What kind of transportation does the reporter use to leave the tribe?\nA. (A) Boat\nB. (B) Bike\nC. (C) Motorcycle\nD. (D) Car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044a60>"
                    ]
                },
                12,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b59121093dcbb2c158a066680770b302dce9feec0a3203eb36bc403df593c169",
            "target_hash": "a3843657b7986d8d8ccb797019a8c4eb24798e4a4499fd1af2f6f7994dd68c9c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 13,
            "doc": {
                "candidates": [
                    "(A) Plan to visit the Penan tribe in the near future",
                    "(B) Plan to live in the Penan tribe in the distant future",
                    "(C) Plan to live in the Penan tribe in the near future",
                    "(D) Plan to visit the Penan tribe in the more distant future"
                ],
                "answer": "(D) Plan to visit the Penan tribe in the more distant future",
                "time_reference": "51:00-52:00",
                "question_type": [
                    "reasoning"
                ],
                "question": "What might the reporter do?",
                "type": "documentary",
                "video": "TiQBTesZUJQ"
            },
            "target": "(D) Plan to visit the Penan tribe in the more distant future",
            "arguments": [
                "What might the reporter do?\nA. (A) Plan to visit the Penan tribe in the near future\nB. (B) Plan to live in the Penan tribe in the distant future\nC. (C) Plan to live in the Penan tribe in the near future\nD. (D) Plan to visit the Penan tribe in the more distant future\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046740>"
                    ]
                },
                13,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a2d1b02d52822fd2fc730edd9ccd49672e0c089158e71b43a3d22cfdbaa22e23",
            "target_hash": "a028146ec0ee47879161804c7166cae579a2f0bb6adab908f86946c6bb0373d4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 14,
            "doc": {
                "candidates": [
                    "(A) Ma Huateng",
                    "(B) Jack Ma",
                    "(C) Li Kaifu",
                    "(D) Robin Li"
                ],
                "answer": "(C) Li Kaifu",
                "time_reference": "13:17-13:17",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Who is the man wearing glasses, a black suit, and a blue patterned tie?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(C) Li Kaifu",
            "arguments": [
                "Who is the man wearing glasses, a black suit, and a blue patterned tie?\nA. (A) Ma Huateng\nB. (B) Jack Ma\nC. (C) Li Kaifu\nD. (D) Robin Li\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047460>"
                    ]
                },
                14,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8ed200028f3350168ce04d86f71481b8f8abba0f1e5fda734deb57fc1f674c19",
            "target_hash": "718b8427d7bad2cd0ab73e95d0fd678d0faf2c28b09f4b906d60b9197ef30f80",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 15,
            "doc": {
                "candidates": [
                    "(A) A technology documentary",
                    "(B) A news report",
                    "(C) A travel documentary",
                    "(D) A travel vlog"
                ],
                "answer": "(A) A technology documentary",
                "time_reference": "00:00-114:16",
                "question_type": [
                    "event understanding"
                ],
                "question": "What type of video is this?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(A) A technology documentary",
            "arguments": [
                "What type of video is this?\nA. (A) A technology documentary\nB. (B) A news report\nC. (C) A travel documentary\nD. (D) A travel vlog\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047d90>"
                    ]
                },
                15,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "60af01fee1dc125a3591a5507992c585cb4b0c294e6fea57c229faf13d809388",
            "target_hash": "06244efa2d238d1729f7d6cf1b14335da81dfb36e5fc67001cef029ccfb6e817",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 16,
            "doc": {
                "candidates": [
                    "(A) Engaged",
                    "(B) Technology",
                    "(C) Embark",
                    "(D) Speed"
                ],
                "answer": "(A) Engaged",
                "time_reference": "22:35-22:35",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is the label on the toggle switch button after the first driver sitting in a truck without driving in chapter 2 'The promise' of the video?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(A) Engaged",
            "arguments": [
                "What is the label on the toggle switch button after the first driver sitting in a truck without driving in chapter 2 'The promise' of the video?\nA. (A) Engaged\nB. (B) Technology\nC. (C) Embark\nD. (D) Speed\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd0e20>"
                    ]
                },
                16,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5863c5f2e6fe985024a8b829ed754ffd2a952d32a519aa5eed6d0d63ef16a4f0",
            "target_hash": "4bf2a9f588a1ae714b32283f093ba7a20d23bc4bafd456667c776c23b720805e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 17,
            "doc": {
                "candidates": [
                    "(A) The scene is filmed in reality",
                    "(B) The scene is a simulated representation of reality",
                    "(C) This scene has neither simulation nor reality",
                    "(D) This scene has both simulation and real-life scenarios"
                ],
                "answer": "(A) The scene is filmed in reality",
                "time_reference": "21:44-25:52",
                "question_type": [
                    "reasoning"
                ],
                "question": "Is the autonomous vehicle driving scene after a man in black said something real or simulated?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(A) The scene is filmed in reality",
            "arguments": [
                "Is the autonomous vehicle driving scene after a man in black said something real or simulated?\nA. (A) The scene is filmed in reality\nB. (B) The scene is a simulated representation of reality\nC. (C) This scene has neither simulation nor reality\nD. (D) This scene has both simulation and real-life scenarios\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2770>"
                    ]
                },
                17,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "835f181f1ec853be50c261186d7bd8bf293e961cba274f1cf0332df2541d6675",
            "target_hash": "a4cf51942267819312e1bea76833c907743647f7ecdaec281947c3d6fddc1b34",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 18,
            "doc": {
                "candidates": [
                    "(A) Colleagues",
                    "(B) Husband and wife",
                    "(C) Interviewer and interviewee",
                    "(D) Brother and sister"
                ],
                "answer": "(C) Interviewer and interviewee",
                "time_reference": "37:00-42:00",
                "question_type": [
                    "reasoning"
                ],
                "question": "What is the relationship between the first two people appear in the video after the appearance of blue words 'TRUCK WASH' on a blue factory?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(C) Interviewer and interviewee",
            "arguments": [
                "What is the relationship between the first two people appear in the video after the appearance of blue words 'TRUCK WASH' on a blue factory?\nA. (A) Colleagues\nB. (B) Husband and wife\nC. (C) Interviewer and interviewee\nD. (D) Brother and sister\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd38b0>"
                    ]
                },
                18,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "be1dfa06545968a2b0f35cd0aaa2738e3e3bbc9fb762382e3c88f010297a0cd4",
            "target_hash": "a2814335b6535657d0976553a7b6fa384aa6501648f15f284131a30ddbbd4b6c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 19,
            "doc": {
                "candidates": [
                    "(A) Around 2006",
                    "(B) Around 1996",
                    "(C) Around 2001",
                    "(D) Around 1991"
                ],
                "answer": "(B) Around 1996",
                "time_reference": "63:10-63:30",
                "question_type": [
                    "event understanding"
                ],
                "question": "Around what year do the blue and red lines intersect?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(B) Around 1996",
            "arguments": [
                "Around what year do the blue and red lines intersect?\nA. (A) Around 2006\nB. (B) Around 1996\nC. (C) Around 2001\nD. (D) Around 1991\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd26b0>"
                    ]
                },
                19,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "09290c391f30b980f2e043c41a95b84cffd0bc793cafbbed282d633168d48e26",
            "target_hash": "3ff7c02f1228b7f0933de01228e28999922c27be50e60be874972c85209aa650",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 20,
            "doc": {
                "candidates": [
                    "(A) People listening to a speaker on stage",
                    "(B) A mentor guiding a student",
                    "(C) Individuals collaborating on a project",
                    "(D) Characters engaging in a group discussion"
                ],
                "answer": "(A) People listening to a speaker on stage",
                "time_reference": "76:11-76:43",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How do the main characters interact with each other after the appearance of book 'Zurked' with a thumb down on the book?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(A) People listening to a speaker on stage",
            "arguments": [
                "How do the main characters interact with each other after the appearance of book 'Zurked' with a thumb down on the book?\nA. (A) People listening to a speaker on stage\nB. (B) A mentor guiding a student\nC. (C) Individuals collaborating on a project\nD. (D) Characters engaging in a group discussion\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0457e0>"
                    ]
                },
                20,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "09aa9b7836d6525d00b790e6b2614f6645a7a504a29039acc271a7b6ded4a7b2",
            "target_hash": "da2a612c7eff753dcca88e43fc5a58a231c6a60cbd838b697628c876727a6bb2",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 21,
            "doc": {
                "candidates": [
                    "(A) Showcasing innovative working automation",
                    "(B) Demonstration of futuristic schools",
                    "(C) Exhibition of advanced labotoray technology",
                    "(D) Smart home scenes display"
                ],
                "answer": "(D) Smart home scenes display",
                "time_reference": "80:05-80:56",
                "question_type": [
                    "event understanding"
                ],
                "question": "Based on the visual cues, what is on the scene before the scene focus on a water glass with white top and gray bottom with words 'Promotional Video' in the upper left corner of the video",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(D) Smart home scenes display",
            "arguments": [
                "Based on the visual cues, what is on the scene before the scene focus on a water glass with white top and gray bottom with words 'Promotional Video' in the upper left corner of the video\nA. (A) Showcasing innovative working automation\nB. (B) Demonstration of futuristic schools\nC. (C) Exhibition of advanced labotoray technology\nD. (D) Smart home scenes display\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046500>"
                    ]
                },
                21,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "21a327249f0a7721ab7c2e96e245bb66f79b4586679b3f1dd445a9a81962cfb6",
            "target_hash": "7467d9b7ca427df424ed2d48760d036f23847fe0d32220f9907c34bee3117ae8",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 22,
            "doc": {
                "candidates": [
                    "(A) Pope Benedict XVI",
                    "(B) Hillary Clinton",
                    "(C) Tell Me Now",
                    "(D) Your News Wire"
                ],
                "answer": "(D) Your News Wire",
                "time_reference": "86:31-86:31",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "Which account's Facebook page is displayed with 28402 likes?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(D) Your News Wire",
            "arguments": [
                "Which account's Facebook page is displayed with 28402 likes?\nA. (A) Pope Benedict XVI\nB. (B) Hillary Clinton\nC. (C) Tell Me Now\nD. (D) Your News Wire\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047220>"
                    ]
                },
                22,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a978d65bcda3704413cef6c77d64af63207ff412a6097c05b2b318a6ec97cbbf",
            "target_hash": "19dd6ac8752af163d8c0b808ef5ec697079a874742d4f12bd630cf66c129e0d9",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 23,
            "doc": {
                "candidates": [
                    "(A) 0",
                    "(B) 2",
                    "(C) 1",
                    "(D) 3"
                ],
                "answer": "(B) 2",
                "time_reference": "32:08-32:19",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "How many people are interviewed in the M.I.T. AI Lab for using AI to learn about cancer detection with mammogram data?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(B) 2",
            "arguments": [
                "How many people are interviewed in the M.I.T. AI Lab for using AI to learn about cancer detection with mammogram data?\nA. (A) 0\nB. (B) 2\nC. (C) 1\nD. (D) 3\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047b50>"
                    ]
                },
                23,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2a1ad17eda3ab2ae7f2dd36ebd1e7e8cf03ef2a4a1b705c608cee190e59195c8",
            "target_hash": "4474ee930ab66e43cf7087501d2bde2bf2de61d92c49d0ee75219ed0f498d475",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 24,
            "doc": {
                "candidates": [
                    "(A) Ensuring safety",
                    "(B) Accompanying and showing courtesy",
                    "(C) Arresting",
                    "(D) Displaying friendliness"
                ],
                "answer": "(C) Arresting",
                "time_reference": "101:21-101:21",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "question": "What goals or objectives are the man holding a woman's arm trying to achieve at 101:21?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(C) Arresting",
            "arguments": [
                "What goals or objectives are the man holding a woman's arm trying to achieve at 101:21?\nA. (A) Ensuring safety\nB. (B) Accompanying and showing courtesy\nC. (C) Arresting\nD. (D) Displaying friendliness\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd0fd0>"
                    ]
                },
                24,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2c009de86ba0acc05bed97733869aa8cced520cdc2484b15b5fe8c5c3d96c41d",
            "target_hash": "8eec06baf4a5263f905cef9ff91b44112b29dd0bd22839f9032ef8e868fc950a",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 25,
            "doc": {
                "candidates": [
                    "(A) Family",
                    "(B) Classmates",
                    "(C) Colleagues",
                    "(D) Strangers"
                ],
                "answer": "(C) Colleagues",
                "time_reference": "103:57-103:58",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "question": "What is the most possible relationships among the people at 103:57?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(C) Colleagues",
            "arguments": [
                "What is the most possible relationships among the people at 103:57?\nA. (A) Family\nB. (B) Classmates\nC. (C) Colleagues\nD. (D) Strangers\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1f90>"
                    ]
                },
                25,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "04e8720fda357a91b187fab4f1e9ce360c38ee84e10ee111effce34737f100d1",
            "target_hash": "f7203c94559d0393fd796ca154cc8a6aab5ded2a59e3caa187601ce90ed69fa4",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 26,
            "doc": {
                "candidates": [
                    "(A) AI technology",
                    "(B) Future cities",
                    "(C) Industrial innovation",
                    "(D) Automation and robotics"
                ],
                "answer": "(A) AI technology",
                "time_reference": "21:37-54:16",
                "question_type": [
                    "event understanding"
                ],
                "question": "Based on the visual elements, can you identify the theme of this video?",
                "type": "documentary",
                "video": "5dZ_lvDgevk"
            },
            "target": "(A) AI technology",
            "arguments": [
                "Based on the visual elements, can you identify the theme of this video?\nA. (A) AI technology\nB. (B) Future cities\nC. (C) Industrial innovation\nD. (D) Automation and robotics\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd30d0>"
                    ]
                },
                26,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4b863f1f9a1be2d34186049b73c3e4a204f0b64868e7d1bc3344b952de848215",
            "target_hash": "a30cc41ca293732cac02b3231787218463373903ccbad89b456498775089c2d3",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 27,
            "doc": {
                "candidates": [
                    "(A) The little lion",
                    "(B) The bat-eared fox",
                    "(C) The ostrich",
                    "(D) The deer"
                ],
                "answer": "(D) The deer",
                "time_reference": "00:52-00:52",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the animal in the first scene that appears after the opening cloud scene?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(D) The deer",
            "arguments": [
                "What is the animal in the first scene that appears after the opening cloud scene?\nA. (A) The little lion\nB. (B) The bat-eared fox\nC. (C) The ostrich\nD. (D) The deer\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3dc0>"
                    ]
                },
                27,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "28f8e3fe4773eb0099f9a613aa8cfbee60a42b1832b571a93ba4dfff1a666a7e",
            "target_hash": "fe753b4590cb0cf7de999f886ac1044fec32c1017ca8dba20537f8f10a249912",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 28,
            "doc": {
                "candidates": [
                    "(A) The deer",
                    "(B) The little lion",
                    "(C) The bat-eared fox",
                    "(D) The ostriches"
                ],
                "answer": "(D) The ostriches",
                "time_reference": "02:07-02:07",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the animal that appears in the picture after the first appearance of a lion?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(D) The ostriches",
            "arguments": [
                "What is the animal that appears in the picture after the first appearance of a lion?\nA. (A) The deer\nB. (B) The little lion\nC. (C) The bat-eared fox\nD. (D) The ostriches\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044700>"
                    ]
                },
                28,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5c1f6181fe4ac4fb7f22721b799dfd41a5a5a49a2665c8464d5f1ab778454990",
            "target_hash": "84caa35272de7abdeee3370c9cd97c3708ffdbd188a7ec5dd97dca862743db23",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 29,
            "doc": {
                "candidates": [
                    "(A) They are sleeping",
                    "(B) They are fighting",
                    "(C) They are mating",
                    "(D) They are hunting"
                ],
                "answer": "(A) They are sleeping",
                "time_reference": "05:50-06:18",
                "question_type": [
                    "reasoning"
                ],
                "question": "What are the two deer seen in the first half of the video doing?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(A) They are sleeping",
            "arguments": [
                "What are the two deer seen in the first half of the video doing?\nA. (A) They are sleeping\nB. (B) They are fighting\nC. (C) They are mating\nD. (D) They are hunting\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045c00>"
                    ]
                },
                29,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4a18cba696832cce3a21e4372e54e52a7288d412020c0a2687b967713c8401fd",
            "target_hash": "7606b8627738eee6f6f25875ab8777a9fb994446086badca5811b2dce2bffa16",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 30,
            "doc": {
                "candidates": [
                    "(A) Grass",
                    "(B) Another fox",
                    "(C) Hand",
                    "(D) Tail"
                ],
                "answer": "(D) Tail",
                "time_reference": "15:46-15:46",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is a bat-eared fox licking in the grass in the middle of the picture?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(D) Tail",
            "arguments": [
                "What is a bat-eared fox licking in the grass in the middle of the picture?\nA. (A) Grass\nB. (B) Another fox\nC. (C) Hand\nD. (D) Tail\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045630>"
                    ]
                },
                30,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "936fa2036391da918bb8d82d0a8e7edc594253ee6fddf5c591436c44628c5696",
            "target_hash": "8c45f71becf0adbb6eee3a3268c18d681648e2ed5c09ca31fbeab6aa56461dca",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 31,
            "doc": {
                "candidates": [
                    "(A) Three lions play with each other",
                    "(B) Three leopard foxes play with each other",
                    "(C) Three bat-eared foxes play with each other",
                    "(D) Three deers play with each other"
                ],
                "answer": "(C) Three bat-eared foxes play with each other",
                "time_reference": "17:52-18:22",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "What are the three animals that appear after two deer eat grass and play with their horns?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) Three bat-eared foxes play with each other",
            "arguments": [
                "What are the three animals that appear after two deer eat grass and play with their horns?\nA. (A) Three lions play with each other\nB. (B) Three leopard foxes play with each other\nC. (C) Three bat-eared foxes play with each other\nD. (D) Three deers play with each other\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047130>"
                    ]
                },
                31,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "aecf2ac8e5ac0c0275006683c3ccaa598d1e10349ec773609a3fa6c0531f84a7",
            "target_hash": "b4d43a3dbc81527a212757b7ab22fee4499d5a1a3b5862f79dd26346e0ae4136",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 32,
            "doc": {
                "candidates": [
                    "(A) Black",
                    "(B) Pink",
                    "(C) Golden",
                    "(D) Blue"
                ],
                "answer": "(C) Golden",
                "time_reference": "19:18-19:18",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is the cloud layer captured for the second time in the video?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) Golden",
            "arguments": [
                "What color is the cloud layer captured for the second time in the video?\nA. (A) Black\nB. (B) Pink\nC. (C) Golden\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd13c0>"
                    ]
                },
                32,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1b20670c16cad080b028d723828ebd2a8ea2a4069e625854f816ce06a5716252",
            "target_hash": "30e4673766fbe4d62784559eeff8a150d4176925e6c1bc40f62f7368bebf642e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 33,
            "doc": {
                "candidates": [
                    "(A) African fish eagles fly in the sky",
                    "(B) African goshawks fly in the sky",
                    "(C) White-bellied sea eagles fly in the sky",
                    "(D) Namaqua doves fly in the sky"
                ],
                "answer": "(B) African goshawks fly in the sky",
                "time_reference": "28:37-29:10",
                "question_type": [
                    "temporal grounding",
                    "event understanding"
                ],
                "question": "What happens from 28:37-29:10?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(B) African goshawks fly in the sky",
            "arguments": [
                "What happens from 28:37-29:10?\nA. (A) African fish eagles fly in the sky\nB. (B) African goshawks fly in the sky\nC. (C) White-bellied sea eagles fly in the sky\nD. (D) Namaqua doves fly in the sky\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd14e0>"
                    ]
                },
                33,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bb1f03a68d3e851a036f68c3afb85f67927b1b515ebb7d9e884c2ba61625c9e9",
            "target_hash": "49c7e91e7f3c927f45d35432de7cd4ac33eb100adaee93fb7aca7a98743c867e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 34,
            "doc": {
                "candidates": [
                    "(A) African goshawks",
                    "(B) Namaqua doves",
                    "(C) Bateleur",
                    "(D) Helmeted guineafowl"
                ],
                "answer": "(C) Bateleur",
                "time_reference": "31:38-31:38",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What animal is one of the five animals that appears in the picture walking on the grass?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) Bateleur",
            "arguments": [
                "What animal is one of the five animals that appears in the picture walking on the grass?\nA. (A) African goshawks\nB. (B) Namaqua doves\nC. (C) Bateleur\nD. (D) Helmeted guineafowl\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd29e0>"
                    ]
                },
                34,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "062ae9520f6b24d4b80538f1aa9968401b49b6225573d9437ad6b3db6f68ab5b",
            "target_hash": "a9414980304a1ce46f3a39db5f52d573035ca44a18e8af29b570a6a6a85d9af2",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 35,
            "doc": {
                "candidates": [
                    "(A) Yellow",
                    "(B) Grey",
                    "(C) Black",
                    "(D) Green"
                ],
                "answer": "(A) Yellow",
                "time_reference": "31:54-31:54",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What color is the grass around the lioness lying alone in the grass?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(A) Yellow",
            "arguments": [
                "What color is the grass around the lioness lying alone in the grass?\nA. (A) Yellow\nB. (B) Grey\nC. (C) Black\nD. (D) Green\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3c40>"
                    ]
                },
                35,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "57ce4345679f9f2caf985d8ac2dff45c30e78f148ed631b56de97cb9cca49bb0",
            "target_hash": "17db5ef3256df7c9256c75ba0516a35208ef27948344ccc3af6325bf80ace62e",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 36,
            "doc": {
                "candidates": [
                    "(A) Ostrich drink water",
                    "(B) Deers eat the grass",
                    "(C) Giraffes eat the grass",
                    "(D) Zebra eat the grass"
                ],
                "answer": "(D) Zebra eat the grass",
                "time_reference": "32:19-32:35",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What happens after two lions lying on the grass enjoying sunshine, and then a bird standing on top of a rock?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(D) Zebra eat the grass",
            "arguments": [
                "What happens after two lions lying on the grass enjoying sunshine, and then a bird standing on top of a rock?\nA. (A) Ostrich drink water\nB. (B) Deers eat the grass\nC. (C) Giraffes eat the grass\nD. (D) Zebra eat the grass\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044b50>"
                    ]
                },
                36,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5c9b5ac79bb58a198c9598a8d2cd747765085f8a56598ca0c86664465ae993e8",
            "target_hash": "4ef815a247e494a0705a3e33c82a579e2f21dc5b8137c5068543f44c4f82c80e",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 37,
            "doc": {
                "candidates": [
                    "(A) Bat-eared fox",
                    "(B) Bateleur",
                    "(C) Greater kudu",
                    "(D) Deer"
                ],
                "answer": "(C) Greater kudu",
                "time_reference": "37:43-37:43",
                "question_type": [
                    "event understanding"
                ],
                "question": "What appears on the screen before five deers eating the grass and then two giraffes walking on the grass?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) Greater kudu",
            "arguments": [
                "What appears on the screen before five deers eating the grass and then two giraffes walking on the grass?\nA. (A) Bat-eared fox\nB. (B) Bateleur\nC. (C) Greater kudu\nD. (D) Deer\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045360>"
                    ]
                },
                37,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6976b913bdd4d89e950c284d5d29b2855f247f864346fa41dd6f32e00150ab62",
            "target_hash": "cdccd01012f08644f6eca438d59079c3f2359088b9ba76d983870cc785a5b6e3",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 38,
            "doc": {
                "candidates": [
                    "(A) Helmeted guineafowl",
                    "(B) Namaqua doves",
                    "(C) African fish eagle",
                    "(D) Bateleur"
                ],
                "answer": "(C) African fish eagle",
                "time_reference": "38:14-38:14",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What is the second other animal that appears after the great kudu?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) African fish eagle",
            "arguments": [
                "What is the second other animal that appears after the great kudu?\nA. (A) Helmeted guineafowl\nB. (B) Namaqua doves\nC. (C) African fish eagle\nD. (D) Bateleur\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044fd0>"
                    ]
                },
                38,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a06e5fc47c6ec616587cfa82314b7de840e12c25c951eb9a8913748866d4647c",
            "target_hash": "01f9b1ab7ef2a6b41c3b5544b7a17c61f30d75a87149bc418b0291fbc99e3940",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 39,
            "doc": {
                "candidates": [
                    "(A) Hippo",
                    "(B) African elephant",
                    "(C) Crocodile",
                    "(D) African fish eagle"
                ],
                "answer": "(A) Hippo",
                "time_reference": "39:26-39:26",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What appears in the water before the scene of a hippo eating grass?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(A) Hippo",
            "arguments": [
                "What appears in the water before the scene of a hippo eating grass?\nA. (A) Hippo\nB. (B) African elephant\nC. (C) Crocodile\nD. (D) African fish eagle\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046e00>"
                    ]
                },
                39,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "03745f7d96672a8c18f2924d6c2232255a8a7324fa150ff8aea59bbb4242d490",
            "target_hash": "dff113febb5969a6ecebcccab01e73ecdd3758c2401bd23d0f0fc93ddc81d240",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 40,
            "doc": {
                "candidates": [
                    "(A) Black-winged stilt",
                    "(B) Crimson-breasted shrike",
                    "(C) Lilac-breasted roller",
                    "(D) Bat-eared fox"
                ],
                "answer": "(C) Lilac-breasted roller",
                "time_reference": "40:51-40:51",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What appears on the screen before the scene where a Nile monitor walks towards the right, making a hissing sound with its tongue, respective to the camera's view?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) Lilac-breasted roller",
            "arguments": [
                "What appears on the screen before the scene where a Nile monitor walks towards the right, making a hissing sound with its tongue, respective to the camera's view?\nA. (A) Black-winged stilt\nB. (B) Crimson-breasted shrike\nC. (C) Lilac-breasted roller\nD. (D) Bat-eared fox\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd0c10>"
                    ]
                },
                40,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6ccec430f4af7f56b8877aefbd9adeaa9ba51ff1d745bd8a3537fc42a8b719f8",
            "target_hash": "3a8bbf557285b3db1f78b857800590157d0fcfbc3e43d8cb8f19a9cdccf5f8ff",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 41,
            "doc": {
                "candidates": [
                    "(A) Namaqua doves",
                    "(B) Helmeted guineafowl",
                    "(C) African fish eagle",
                    "(D) Nile monitor"
                ],
                "answer": "(D) Nile monitor",
                "time_reference": "41:02-41:02",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What appears after a lilac-breasted roller shows up for the first time?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(D) Nile monitor",
            "arguments": [
                "What appears after a lilac-breasted roller shows up for the first time?\nA. (A) Namaqua doves\nB. (B) Helmeted guineafowl\nC. (C) African fish eagle\nD. (D) Nile monitor\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd0d60>"
                    ]
                },
                41,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6587edad97c21f86d9388718310ed28a3ee7b42bf4cc1a2df7b03c45220e5699",
            "target_hash": "d8fa7f6be7d76ed8d47d873abce500eac6d68a68bf4e61fca2cf948af6e35376",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 42,
            "doc": {
                "candidates": [
                    "(A) Tiger",
                    "(B) Bat-eared fox",
                    "(C) African ground squirrel",
                    "(D) African openbill"
                ],
                "answer": "(C) African ground squirrel",
                "time_reference": "43:00-43:27",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What animal licks its fur and then looks straight to the camera?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(C) African ground squirrel",
            "arguments": [
                "What animal licks its fur and then looks straight to the camera?\nA. (A) Tiger\nB. (B) Bat-eared fox\nC. (C) African ground squirrel\nD. (D) African openbill\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1570>"
                    ]
                },
                42,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "78556105e00f75dcb6267b9d45711e5ec732e7945f4545b704d904c83367e571",
            "target_hash": "689f314f0d4d8c2d392efdddab526fa9c51f55619fbcce3515bbf6fc7d997b01",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 43,
            "doc": {
                "candidates": [
                    "(A) Leopards and deers run",
                    "(B) Lions and zebra run",
                    "(C) Lions and deers run",
                    "(D) Zebra and deers run"
                ],
                "answer": "(D) Zebra and deers run",
                "time_reference": "53:26-53:52",
                "question_type": [
                    "event understanding"
                ],
                "question": "What happens after the series of close-up shots of lions?",
                "type": "documentary",
                "video": "GcRKREorGSc"
            },
            "target": "(D) Zebra and deers run",
            "arguments": [
                "What happens after the series of close-up shots of lions?\nA. (A) Leopards and deers run\nB. (B) Lions and zebra run\nC. (C) Lions and deers run\nD. (D) Zebra and deers run\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd38e0>"
                    ]
                },
                43,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d72014604a2f62a0a52b0d94221b3ce6bc3c5394655e21e70b7b541c98a156ed",
            "target_hash": "1a4b109e3be03fc9572bae66b10b73bd14c6844e4f012ca610c843b51c5d5c22",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 44,
            "doc": {
                "candidates": [
                    "(A) Natural scenery",
                    "(B) History",
                    "(C) National geographic",
                    "(D) Humanities"
                ],
                "answer": "(C) National geographic",
                "time_reference": "00:00-44:24",
                "question_type": [
                    "event understanding"
                ],
                "question": "What type of video is this?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(C) National geographic",
            "arguments": [
                "What type of video is this?\nA. (A) Natural scenery\nB. (B) History\nC. (C) National geographic\nD. (D) Humanities\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044eb0>"
                    ]
                },
                44,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8c20408f8c1cf0483c84ca4f8d94c5bc8f1e85b2d24bf87b53053febc5fd6f75",
            "target_hash": "097d4da7126d293aea5741ef5f85dce4c2b0e9b71d87107c0e0dbeb16865adcb",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 45,
            "doc": {
                "candidates": [
                    "(A) Golden",
                    "(B) Black",
                    "(C) Red",
                    "(D) Blue"
                ],
                "answer": "(B) Black",
                "time_reference": "30:00-30:10",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color watch do respondents wearing blue short sleeves and glasses wear?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(B) Black",
            "arguments": [
                "What color watch do respondents wearing blue short sleeves and glasses wear?\nA. (A) Golden\nB. (B) Black\nC. (C) Red\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045a50>"
                    ]
                },
                45,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6223f1686446b3a2ebe273560dc37163bfd22278342ef6e12429b2fb2887e38b",
            "target_hash": "bb359097fe322499e4d46a7bd79f43f2df73e9efbfad5cdf87cf32d127d6a8d5",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 46,
            "doc": {
                "candidates": [
                    "(A) Black",
                    "(B) Orange",
                    "(C) Blue",
                    "(D) Red"
                ],
                "answer": "(B) Orange",
                "time_reference": "34:00-34:10",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color hats do children wearing yellow short sleeves wear?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(B) Orange",
            "arguments": [
                "What color hats do children wearing yellow short sleeves wear?\nA. (A) Black\nB. (B) Orange\nC. (C) Blue\nD. (D) Red\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045990>"
                    ]
                },
                46,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f67223c82e9b99b993103aba7baa88c4191a20b683a58ab556a4e276b980e8c4",
            "target_hash": "1d5e0e9af1a6546cb8118be7370935727ec3fc75f5a4a1739b481b79605c418d",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 47,
            "doc": {
                "candidates": [
                    "(A) Friendship",
                    "(B) Stranger relationship",
                    "(C) Mother-child relationship",
                    "(D) Teacher-student relationship"
                ],
                "answer": "(D) Teacher-student relationship",
                "time_reference": "34:00-35:00",
                "question_type": [
                    "reasoning"
                ],
                "question": "What is the relationship between the lady in blue clothes and the kid in yellow clothes?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(D) Teacher-student relationship",
            "arguments": [
                "What is the relationship between the lady in blue clothes and the kid in yellow clothes?\nA. (A) Friendship\nB. (B) Stranger relationship\nC. (C) Mother-child relationship\nD. (D) Teacher-student relationship\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0475e0>"
                    ]
                },
                47,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d9065ca229319975bc848bc915642ddab3615e342486f6b91a423a95f05b540e",
            "target_hash": "d5757cb31231187fad53983f39b76b3ce907cc09cb7d101ede68290e600cd54c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 48,
            "doc": {
                "candidates": [
                    "(A) Telescope",
                    "(B) Necklace",
                    "(C) Watercup",
                    "(D) Camera"
                ],
                "answer": "(A) Telescope",
                "time_reference": "41:07-41:07",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is hanging around the neck of the girl observing birds in the forest in the video?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(A) Telescope",
            "arguments": [
                "What is hanging around the neck of the girl observing birds in the forest in the video?\nA. (A) Telescope\nB. (B) Necklace\nC. (C) Watercup\nD. (D) Camera\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1330>"
                    ]
                },
                48,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "963c2d84e4a91f540d53fe21a81bed00c3d370cecd108171e2c1fb69ca7e443d",
            "target_hash": "2b313197af7b720ea50fce97b02f0c73e0d505d63df8185f8699eddbd4439ba4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 49,
            "doc": {
                "candidates": [
                    "(A) Horse",
                    "(B) Cow",
                    "(C) Bird",
                    "(D) Chicken"
                ],
                "answer": "(D) Chicken",
                "time_reference": "42:00-42:00",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "question": "What animal is this at 42:00?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(D) Chicken",
            "arguments": [
                "What animal is this at 42:00?\nA. (A) Horse\nB. (B) Cow\nC. (C) Bird\nD. (D) Chicken\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1cf0>"
                    ]
                },
                49,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e150b3243fc94d2c96cf7427b5e5c124f86a04cdb2cabab3d4f72072089c8bb3",
            "target_hash": "18f9b48aacb78fa17509426752cf9bc7212fb64a88f3f15c18fbf3cea7f37003",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 50,
            "doc": {
                "candidates": [
                    "(A) Bird",
                    "(B) Dragonfly",
                    "(C) Butterfly",
                    "(D) Lizard"
                ],
                "answer": "(B) Dragonfly",
                "time_reference": "42:08-42:08",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What animal appears after the interview with Karen Sim and Jeffrey Chong?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(B) Dragonfly",
            "arguments": [
                "What animal appears after the interview with Karen Sim and Jeffrey Chong?\nA. (A) Bird\nB. (B) Dragonfly\nC. (C) Butterfly\nD. (D) Lizard\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1ab0>"
                    ]
                },
                50,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "473c9119d107e27730a03ee406f26cb60a5139cb0c120d442744ae4b3560764e",
            "target_hash": "0bacebd1aec1bab0876068086bcba843bd957a8108c4687e0a45fc4887804e19",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 51,
            "doc": {
                "candidates": [
                    "(A) Whale",
                    "(B) Crocodile",
                    "(C) Lizard",
                    "(D) Hippo"
                ],
                "answer": "(C) Lizard",
                "time_reference": "42:50-42:50",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "What animal is shown in the scene that follows the scene Karen Sim submits a sighting photo and earns a badge on the phone?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(C) Lizard",
            "arguments": [
                "What animal is shown in the scene that follows the scene Karen Sim submits a sighting photo and earns a badge on the phone?\nA. (A) Whale\nB. (B) Crocodile\nC. (C) Lizard\nD. (D) Hippo\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1ed0>"
                    ]
                },
                51,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "be356cc1ffd8c64c04dd505281f932624c2525b94513a4523c872fd26d8baa2b",
            "target_hash": "40ba8802bc6dcfac268c1bda117e4579fee1001324f754b8f9fd55de2f4bcd17",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 52,
            "doc": {
                "candidates": [
                    "(A) Black",
                    "(B) Dark blue",
                    "(C) Red",
                    "(D) Yellow"
                ],
                "answer": "(B) Dark blue",
                "time_reference": "17:12-17:30",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is the device that Mark Wong wears to monitor his health?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(B) Dark blue",
            "arguments": [
                "What color is the device that Mark Wong wears to monitor his health?\nA. (A) Black\nB. (B) Dark blue\nC. (C) Red\nD. (D) Yellow\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045210>"
                    ]
                },
                52,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "303bccc41c739a23e5bfd32b856a65e58d4e391fa42675f71de4f91834b5a58a",
            "target_hash": "596d37b8332506f81fe2e6705fa6cbe9b4a2e614e339bc345b2972a7baa77d21",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 53,
            "doc": {
                "candidates": [
                    "(A) Carp",
                    "(B) Jellyfish",
                    "(C) Frog",
                    "(D) Squid"
                ],
                "answer": "(A) Carp",
                "time_reference": "20:33-20:33",
                "question_type": [
                    "entity recognition",
                    "event understanding",
                    "key information retrieval"
                ],
                "question": "When the video discusses water reservoir, which animal lives in water appear?",
                "type": "documentary",
                "video": "xi6r3hZe5Tg"
            },
            "target": "(A) Carp",
            "arguments": [
                "When the video discusses water reservoir, which animal lives in water appear?\nA. (A) Carp\nB. (B) Jellyfish\nC. (C) Frog\nD. (D) Squid\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045e70>"
                    ]
                },
                53,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a26ab69c925e2d25d11eb4dd8e934e49a8be6b419192821700bf148e07b3dd82",
            "target_hash": "43da23c68a6f78d5b47a4ed7ed753353e66a849b139bbba23bbb5861fb0841d7",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 54,
            "doc": {
                "candidates": [
                    "(A) Wheel",
                    "(B) Bearing",
                    "(C) Engine",
                    "(D) Brake caliper"
                ],
                "answer": "(D) Brake caliper",
                "time_reference": "00:43-00:43",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the yellow semi-circular tool used by black clad engineers in cars?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) Brake caliper",
            "arguments": [
                "What is the yellow semi-circular tool used by black clad engineers in cars?\nA. (A) Wheel\nB. (B) Bearing\nC. (C) Engine\nD. (D) Brake caliper\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044cd0>"
                    ]
                },
                54,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c5f3f3287c9249872a19006eb8c7519f9f18cd96d42762010f5860b01143cbeb",
            "target_hash": "a234e3df166056ab0b70046eac542ce3fca1adbcf9aee5c753506f082738b6fd",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 55,
            "doc": {
                "candidates": [
                    "(A) Maybach",
                    "(B) AMG GT",
                    "(C) C class",
                    "(D) E class"
                ],
                "answer": "(A) Maybach",
                "time_reference": "01:20-01:20",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What kind of Mercedes-Benz car is this black one?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) Maybach",
            "arguments": [
                "What kind of Mercedes-Benz car is this black one?\nA. (A) Maybach\nB. (B) AMG GT\nC. (C) C class\nD. (D) E class\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0478e0>"
                    ]
                },
                55,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cd697f8b89bc1bcbf954895d44fcdbc7576bd2e8723302a0f26450ad6e88030d",
            "target_hash": "7e9c86109a7badd5020f06bd34f76b6fb2a5e39a5aa58a48bf5758564bf661ce",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 56,
            "doc": {
                "candidates": [
                    "(A) They replace the exhaust pipe of the car",
                    "(B) They replace the tail lights of the car",
                    "(C) They change tires on cars",
                    "(D) They replace the brake pads on the car"
                ],
                "answer": "(A) They replace the exhaust pipe of the car",
                "time_reference": "04:45-05:25",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "question": "What happens from 04:45-05:25?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) They replace the exhaust pipe of the car",
            "arguments": [
                "What happens from 04:45-05:25?\nA. (A) They replace the exhaust pipe of the car\nB. (B) They replace the tail lights of the car\nC. (C) They change tires on cars\nD. (D) They replace the brake pads on the car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd0f40>"
                    ]
                },
                56,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bbd0d586e4532290e85fbb4b0646205535580de0b018580dd3e6287dbcb39cba",
            "target_hash": "90f21c1e7add237cace46a53f935b5c2bf3710e390ebca2a5a3dde30dcad812e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 57,
            "doc": {
                "candidates": [
                    "(A) He is replacing the exhaust pipe on his car",
                    "(B) He is changing the tires on the car",
                    "(C) He is replacing the brake pads on the car",
                    "(D) He is replacing the wheels on the car"
                ],
                "answer": "(C) He is replacing the brake pads on the car",
                "time_reference": "10:20-11:00",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is Samir Boulahya doing when his name first appears?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(C) He is replacing the brake pads on the car",
            "arguments": [
                "What is Samir Boulahya doing when his name first appears?\nA. (A) He is replacing the exhaust pipe on his car\nB. (B) He is changing the tires on the car\nC. (C) He is replacing the brake pads on the car\nD. (D) He is replacing the wheels on the car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd20e0>"
                    ]
                },
                57,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cee0e721fb860087affd0815e3ebef55eb38e40d0745f148635651f8b989d45a",
            "target_hash": "0a042e0e3ed064750d1ac3d25e49b577d5bc83569f794fda61c87763c4e5bbc5",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 58,
            "doc": {
                "candidates": [
                    "(A) Black",
                    "(B) Red",
                    "(C) Blue",
                    "(D) Green"
                ],
                "answer": "(D) Green",
                "time_reference": "15:39-15:39",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color are the gloves when cleaning parts?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) Green",
            "arguments": [
                "What color are the gloves when cleaning parts?\nA. (A) Black\nB. (B) Red\nC. (C) Blue\nD. (D) Green\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1690>"
                    ]
                },
                58,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "92d2bc316d4c077d3d7f954bbc0e83d4e5782e3f47cd6e581ed5e94dd70cab28",
            "target_hash": "1467faa1684c871eccb61d0fe03dff196c5c3cb3e35d92428a306428f10f86fc",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 59,
            "doc": {
                "candidates": [
                    "(A) Adhesive tape",
                    "(B) Molds",
                    "(C) Boxs",
                    "(D) Tires"
                ],
                "answer": "(B) Molds",
                "time_reference": "16:00-16:00",
                "question_type": [
                    "event understanding",
                    "reasoning"
                ],
                "question": "What is the item placed on the shelf after cleaning?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(B) Molds",
            "arguments": [
                "What is the item placed on the shelf after cleaning?\nA. (A) Adhesive tape\nB. (B) Molds\nC. (C) Boxs\nD. (D) Tires\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3910>"
                    ]
                },
                59,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "aa390622e3a967780b887ea36a26e578ba31d4e309dc847c1b5f1e3965b677bd",
            "target_hash": "ca03c5d55002557ef92b4d10648bb187e1d552e6072f22629c7c07f702ae1b5d",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 60,
            "doc": {
                "candidates": [
                    "(A) He is checking the vehicle chassis",
                    "(B) He is checking the vehicle interior",
                    "(C) He is checking the vehicle windows",
                    "(D) He is checking the vehicle headlights"
                ],
                "answer": "(A) He is checking the vehicle chassis",
                "time_reference": "33:00-33:30",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What does a man wearing a black suit, black glasses, and a beard do?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) He is checking the vehicle chassis",
            "arguments": [
                "What does a man wearing a black suit, black glasses, and a beard do?\nA. (A) He is checking the vehicle chassis\nB. (B) He is checking the vehicle interior\nC. (C) He is checking the vehicle windows\nD. (D) He is checking the vehicle headlights\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045570>"
                    ]
                },
                60,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2fddbc29b4ac2246f3ae428d362c6817f252c14857922f213b56c396967f0cd0",
            "target_hash": "58c1a150e28840b79834f2eb902ec7215d1150534f5b835e8c6402344660b60a",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 61,
            "doc": {
                "candidates": [
                    "(A) Cleaning the seats",
                    "(B) Restoring the seats",
                    "(C) Adjusting the seat",
                    "(D) Stripping the seats"
                ],
                "answer": "(D) Stripping the seats",
                "time_reference": "35:00-36:00",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "What job is the man in red responsible for?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) Stripping the seats",
            "arguments": [
                "What job is the man in red responsible for?\nA. (A) Cleaning the seats\nB. (B) Restoring the seats\nC. (C) Adjusting the seat\nD. (D) Stripping the seats\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044490>"
                    ]
                },
                61,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5eb159795b6c5a12de702700cd255e65cdddb94164fb7ccecd23ea49839d6547",
            "target_hash": "dd92aa752d12c172bb7359ec6f22a2dc57a17b5789bda4b2728975bad6046a39",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 62,
            "doc": {
                "candidates": [
                    "(A) Scissors",
                    "(B) Knife",
                    "(C) Pen",
                    "(D) Ruler"
                ],
                "answer": "(A) Scissors",
                "time_reference": "36:55-36:55",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "What tools does a woman wearing black glasses and silver earrings first use to work?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) Scissors",
            "arguments": [
                "What tools does a woman wearing black glasses and silver earrings first use to work?\nA. (A) Scissors\nB. (B) Knife\nC. (C) Pen\nD. (D) Ruler\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046ce0>"
                    ]
                },
                62,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f0ead3744b550768f9ca86be6319cb28b34e68742b74112aecd0658bf2e43cf8",
            "target_hash": "1e2678b0bd2ddf092006e309d0efa841ad7e99ff615c0ee578bfed35eb91ed99",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 63,
            "doc": {
                "candidates": [
                    "(A) Assemble tires",
                    "(B) Assemble the dashboard",
                    "(C) Assemble skylight",
                    "(D) Assemble the seat"
                ],
                "answer": "(B) Assemble the dashboard",
                "time_reference": "40:00-40:50",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "question": "What is this man doing from 40:00-40:50?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(B) Assemble the dashboard",
            "arguments": [
                "What is this man doing from 40:00-40:50?\nA. (A) Assemble tires\nB. (B) Assemble the dashboard\nC. (C) Assemble skylight\nD. (D) Assemble the seat\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047cd0>"
                    ]
                },
                63,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "79f4add5ca30e6f92fa15ee4c8af896463af2d551e5e253d396bf5352dfe3975",
            "target_hash": "bb61c49728e5e10ff47cebbda85dcbb50a3a38e4baf512a84727c136fd0cac5e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 64,
            "doc": {
                "candidates": [
                    "(A) Purple",
                    "(B) Black",
                    "(C) Red",
                    "(D) Blue"
                ],
                "answer": "(D) Blue",
                "time_reference": "45:16-45:16",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What is the main color of the car interior?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) Blue",
            "arguments": [
                "What is the main color of the car interior?\nA. (A) Purple\nB. (B) Black\nC. (C) Red\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1a50>"
                    ]
                },
                64,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9fe13faf9e11b92a7a6f1ddad86b38d29752da87f4a944c84e4b07068c475e1a",
            "target_hash": "2af8475f8d12112df3b1487762f27a4f3153636b07666dd9be3936b9d3a38176",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 65,
            "doc": {
                "candidates": [
                    "(A) He drives in circles",
                    "(B) He drives uphill",
                    "(C) He drives back to the factory",
                    "(D) He drives down the street"
                ],
                "answer": "(C) He drives back to the factory",
                "time_reference": "47:30-48:00",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How does Klaus Pachall drive after being interviewed?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(C) He drives back to the factory",
            "arguments": [
                "How does Klaus Pachall drive after being interviewed?\nA. (A) He drives in circles\nB. (B) He drives uphill\nC. (C) He drives back to the factory\nD. (D) He drives down the street\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2230>"
                    ]
                },
                65,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f5fe7f0d1f198f0223d3f13674a53e98ba93bd9504cae5c9ac085610ff3622ee",
            "target_hash": "575c2c5c5bf1a90b940f952998d24461d826b97442ec07a9903b73c82ccc0018",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 66,
            "doc": {
                "candidates": [
                    "(A) He changes the interior",
                    "(B) He changes the brake pads",
                    "(C) He brushes the car",
                    "(D) He changes the tire"
                ],
                "answer": "(C) He brushes the car",
                "time_reference": "48:00-48:50",
                "question_type": [
                    "event understanding"
                ],
                "question": "What does the man do after introducing a ton of remover liquid stored in white plastic buckets on a shelf?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(C) He brushes the car",
            "arguments": [
                "What does the man do after introducing a ton of remover liquid stored in white plastic buckets on a shelf?\nA. (A) He changes the interior\nB. (B) He changes the brake pads\nC. (C) He brushes the car\nD. (D) He changes the tire\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3130>"
                    ]
                },
                66,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "77ef2c8c60400e08f08c858dd85bdbd042d13844c0cb5583f9b356f22d573bef",
            "target_hash": "0d0136de31042a80c3b4da3867813194bf2d77bdf7157607f79c0963b772be0e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 67,
            "doc": {
                "candidates": [
                    "(A) The mechanics drives back to the factory",
                    "(B) The mechanics changes the tire",
                    "(C) The mechanics brushes the car",
                    "(D) The mechanics fixes the wheel with a rope"
                ],
                "answer": "(D) The mechanics fixes the wheel with a rope",
                "time_reference": "49:00-49:05",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is the second last event that happens in the video?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) The mechanics fixes the wheel with a rope",
            "arguments": [
                "What is the second last event that happens in the video?\nA. (A) The mechanics drives back to the factory\nB. (B) The mechanics changes the tire\nC. (C) The mechanics brushes the car\nD. (D) The mechanics fixes the wheel with a rope\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044940>"
                    ]
                },
                67,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8b10fe71b0d46eec7a876f1b661c2a94c63d791928aff64a5c88f1aaab209036",
            "target_hash": "3e7ed02845327b6fec6b3640bf5dad8b30a19e49180d98815e7b722c24a46815",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 68,
            "doc": {
                "candidates": [
                    "(A) 24",
                    "(B) 240",
                    "(C) 12",
                    "(D) 120"
                ],
                "answer": "(B) 240",
                "time_reference": "04:32-04:32",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is the serial number of the car that Samir Boulahya works on?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(B) 240",
            "arguments": [
                "What is the serial number of the car that Samir Boulahya works on?\nA. (A) 24\nB. (B) 240\nC. (C) 12\nD. (D) 120\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044bb0>"
                    ]
                },
                68,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a0fb1dcff4363e66cba324789f267a4007c5fa51c5cca8b20824352315c4c16e",
            "target_hash": "b1b1151e8b0f296beddfb21de10f4bde20bfeb4608d57cc8c03b28944cc62068",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 69,
            "doc": {
                "candidates": [
                    "(A) Blue",
                    "(B) Black",
                    "(C) Red",
                    "(D) Purple"
                ],
                "answer": "(A) Blue",
                "time_reference": "04:47-04:47",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "question": "What color gloves does the mechanic, who removes the standard exhaust of a car, wear?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) Blue",
            "arguments": [
                "What color gloves does the mechanic, who removes the standard exhaust of a car, wear?\nA. (A) Blue\nB. (B) Black\nC. (C) Red\nD. (D) Purple\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0462f0>"
                    ]
                },
                69,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2aff2b46cd5293b88016ab1e5c1592463ec3acae823dbaef0c98425edd482d74",
            "target_hash": "d1c056af5b817d5e50d5a991afaee433ec04e7c15ec4caa84c4c00621c7cb2e9",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 70,
            "doc": {
                "candidates": [
                    "(A) Silver gray",
                    "(B) Black",
                    "(C) White",
                    "(D) Red"
                ],
                "answer": "(A) Silver gray",
                "time_reference": "05:23-05:23",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is the car that mechanics are changing from the standard exhaust to the valve-controlled exhaust?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) Silver gray",
            "arguments": [
                "What color is the car that mechanics are changing from the standard exhaust to the valve-controlled exhaust?\nA. (A) Silver gray\nB. (B) Black\nC. (C) White\nD. (D) Red\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047040>"
                    ]
                },
                70,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f2d27e8c98103ac58fcf7d1d6b0b9f72d773d7ad37f14642660cd2ddf73ae915",
            "target_hash": "1354d940b0eba3d3fd6c7328e4901192f8c1ed99add81b86b39d826f374e2ddd",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 71,
            "doc": {
                "candidates": [
                    "(A) Numb",
                    "(B) Cry",
                    "(C) Angry",
                    "(D) Laugh"
                ],
                "answer": "(D) Laugh",
                "time_reference": "06:01-06:01",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "question": "What's this man's expression like at 06:01?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) Laugh",
            "arguments": [
                "What's this man's expression like at 06:01?\nA. (A) Numb\nB. (B) Cry\nC. (C) Angry\nD. (D) Laugh\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047dc0>"
                    ]
                },
                71,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6bd445b197d3bd713caa5557e468e592a79f9f0b59c373e5cd1887c7b6e30d92",
            "target_hash": "9cb774f75c827a3f01dd39fbcf60653eb59db91aae9968df0d3237f55cafa90f",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 72,
            "doc": {
                "candidates": [
                    "(A) Introducing BRABUS's modification of BMW cars. This video records the process of BRABUS's modification of Mercedes Benz E Class",
                    "(B) Introducing BRABUS's modification of Audi cars. This video records the process of BRABUS's modification of Mercedes Benz G Class",
                    "(C) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz E Class",
                    "(D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class"
                ],
                "answer": "(D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class",
                "time_reference": "00:00-49:19",
                "question_type": [
                    "summarization"
                ],
                "question": "What does this documentary mainly introduce?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class",
            "arguments": [
                "What does this documentary mainly introduce?\nA. (A) Introducing BRABUS's modification of BMW cars. This video records the process of BRABUS's modification of Mercedes Benz E Class\nB. (B) Introducing BRABUS's modification of Audi cars. This video records the process of BRABUS's modification of Mercedes Benz G Class\nC. (C) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz E Class\nD. (D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1b40>"
                    ]
                },
                72,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4b66ba1555bf1a308d26ed8012d327aacea701f4a3ad3d57d8a789420e8ab0ba",
            "target_hash": "14c724d8aab35925c323e1f828a8db88ab828f7dc22e8a5367fd39bc8cabe472",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 73,
            "doc": {
                "candidates": [
                    "(A) White",
                    "(B) Red",
                    "(C) Black",
                    "(D) Blue"
                ],
                "answer": "(A) White",
                "time_reference": "06:09-06:09",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is the steering wheel?",
                "type": "documentary",
                "video": "Z4HGQL_McDQ"
            },
            "target": "(A) White",
            "arguments": [
                "What color is the steering wheel?\nA. (A) White\nB. (B) Red\nC. (C) Black\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2590>"
                    ]
                },
                73,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "43477cc61079486f5efb6678e670f2eee2be737ca58c30639eddbc95761e5980",
            "target_hash": "671e4e48ceab719b952bd0c36eb44f6ecce11c1ff999800afe792a8f495c065b",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 74,
            "doc": {
                "candidates": [
                    "(A) Joe Biden",
                    "(B) Vladimir Putin",
                    "(C) Two soldiers",
                    "(D) Donald John Trump"
                ],
                "answer": "(B) Vladimir Putin",
                "time_reference": "00:40-00:45",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Who first comes out of the golden door?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(B) Vladimir Putin",
            "arguments": [
                "Who first comes out of the golden door?\nA. (A) Joe Biden\nB. (B) Vladimir Putin\nC. (C) Two soldiers\nD. (D) Donald John Trump\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3490>"
                    ]
                },
                74,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "65c7542e1d339bc192802ede6cc8c042fe91c64729e7dfd311a1fc67a290d927",
            "target_hash": "bd0b0351d85b23d2ddb833e8a6311d3baaf3dc7a836492736131098f3ecdc8db",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 75,
            "doc": {
                "candidates": [
                    "(A) 5",
                    "(B) 1",
                    "(C) 9",
                    "(D) 4"
                ],
                "answer": "(A) 5",
                "time_reference": "00:53-01:26",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many flags of the Russian Federation are there on the stage when Putin delivers the first speech?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(A) 5",
            "arguments": [
                "How many flags of the Russian Federation are there on the stage when Putin delivers the first speech?\nA. (A) 5\nB. (B) 1\nC. (C) 9\nD. (D) 4\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044ca0>"
                    ]
                },
                75,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6d5c4cbf07e8bb997702a32c8953747f5cb8dae99fd78d3c6032f8b7a4eee88d",
            "target_hash": "c99ecb129251ed73da1079670412a26a12d6e8986f0acfc969979db6c942bbf4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 76,
            "doc": {
                "candidates": [
                    "(A) Blue",
                    "(B) Black",
                    "(C) Wine red",
                    "(D) White"
                ],
                "answer": "(C) Wine red",
                "time_reference": "01:47-01:53",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is Putin's tie between the interview with Antony Blinkoen and interview with Marie Yovanovitch?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(C) Wine red",
            "arguments": [
                "What color is Putin's tie between the interview with Antony Blinkoen and interview with Marie Yovanovitch?\nA. (A) Blue\nB. (B) Black\nC. (C) Wine red\nD. (D) White\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045930>"
                    ]
                },
                76,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "38822fd9c9d78986301d122ba53e46df6f8494a66bfb6e156ec6439954627be9",
            "target_hash": "ffa099135caf533cd031d964ee37ea699c80a2916f955d8f1c9d2de39bd3064a",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 77,
            "doc": {
                "candidates": [
                    "(A) They didn't do anything",
                    "(B) They applauded",
                    "(C) They squatted down",
                    "(D) They stood up"
                ],
                "answer": "(D) They stood up",
                "time_reference": "07:57-08:01",
                "question_type": [
                    "event understanding"
                ],
                "question": "What did the audience do first after the soldiers wearing red hats shouted?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) They stood up",
            "arguments": [
                "What did the audience do first after the soldiers wearing red hats shouted?\nA. (A) They didn't do anything\nB. (B) They applauded\nC. (C) They squatted down\nD. (D) They stood up\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046650>"
                    ]
                },
                77,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "29921a7601f3dc1054df074800faa2f09c84c4da3abca3e311d8c2edb2f17c1c",
            "target_hash": "9a361a152b602c256415aaef391d091e306b909f256b5bde243588a81b301551",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 78,
            "doc": {
                "candidates": [
                    "(A) Marie Yovanovitch",
                    "(B) John Bolton",
                    "(C) Fiona Hill",
                    "(D) Heather Conley"
                ],
                "answer": "(D) Heather Conley",
                "time_reference": "09:17-09:22",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Who is the lady in green who is being interviewed?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) Heather Conley",
            "arguments": [
                "Who is the lady in green who is being interviewed?\nA. (A) Marie Yovanovitch\nB. (B) John Bolton\nC. (C) Fiona Hill\nD. (D) Heather Conley\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047160>"
                    ]
                },
                78,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "789651453a08aaf50ed27f28de9603672a207f4c8e79cdb0f95c17061cc23d35",
            "target_hash": "7733490d2b26030bd856311a51368c6833a98f9b892fcdf0293e78dfe2a77003",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 79,
            "doc": {
                "candidates": [
                    "(A) Flag of the Russian Federation",
                    "(B) Flag of the United States of America",
                    "(C) A piece of white cloth",
                    "(D) A piece of black cloth"
                ],
                "answer": "(B) Flag of the United States of America",
                "time_reference": "12:27-12:33",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What did the man use to cover the sculpture's face?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(B) Flag of the United States of America",
            "arguments": [
                "What did the man use to cover the sculpture's face?\nA. (A) Flag of the Russian Federation\nB. (B) Flag of the United States of America\nC. (C) A piece of white cloth\nD. (D) A piece of black cloth\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0475b0>"
                    ]
                },
                79,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f99a7135d65a857036a71288dd9d9dee514b48dacfc81f2a59c897db94ba9c74",
            "target_hash": "8d76b8ac20a38f056faeee0820ea8cd61b68103a3e508c367fa7cbb8bebb4f63",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 80,
            "doc": {
                "candidates": [
                    "(A) 4",
                    "(B) 3",
                    "(C) 2",
                    "(D) 1"
                ],
                "answer": "(C) 2",
                "time_reference": "00:55-01:08",
                "question_type": [
                    "entity recognition"
                ],
                "question": "When Putin spoke for the first time in the video, how many microphones were on the stage?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(C) 2",
            "arguments": [
                "When Putin spoke for the first time in the video, how many microphones were on the stage?\nA. (A) 4\nB. (B) 3\nC. (C) 2\nD. (D) 1\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1c60>"
                    ]
                },
                80,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6b6991ada243477ab4f8a8202cd0684720854a88e466aa36f79c829b3270f2a0",
            "target_hash": "af0f32bb40559440c5515f57dfbbfb2f51ecb56b390b4bcf1dddb14b635b6eab",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 81,
            "doc": {
                "candidates": [
                    "(A) 2",
                    "(B) 4",
                    "(C) 1",
                    "(D) 0"
                ],
                "answer": "(C) 1",
                "time_reference": "01:00-01:05",
                "question_type": [
                    "entity recognition"
                ],
                "question": "During Putin's first speech on stage in the video, how many women appeared when the camera panned offstage?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(C) 1",
            "arguments": [
                "During Putin's first speech on stage in the video, how many women appeared when the camera panned offstage?\nA. (A) 2\nB. (B) 4\nC. (C) 1\nD. (D) 0\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd28f0>"
                    ]
                },
                81,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4f0104c2f254fa8c28c120d8294269a2c2e56fa5cd8d4ae1820b1a1abdd5bb2e",
            "target_hash": "b0173dff8c7ec3413537675d4593b8d745fd673570d4d180edce7cb3b9cd7950",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 82,
            "doc": {
                "candidates": [
                    "(A) Trump and Biden",
                    "(B) Obama and Putin",
                    "(C) Obama and Trump",
                    "(D) Obama and Biden"
                ],
                "answer": "(D) Obama and Biden",
                "time_reference": "23:00-23:15",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Who are the two people who appear in the next frame after Evan Osnos finishes speaking?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) Obama and Biden",
            "arguments": [
                "Who are the two people who appear in the next frame after Evan Osnos finishes speaking?\nA. (A) Trump and Biden\nB. (B) Obama and Putin\nC. (C) Obama and Trump\nD. (D) Obama and Biden\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd37f0>"
                    ]
                },
                82,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5e98bd4beadd36b723da19e67da0485c916ae384a04fec2220cd62d2cd080801",
            "target_hash": "e12bd72aaa5cd4082ea0a7d220f665ea043abb41492766952139a75065aaa705",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 83,
            "doc": {
                "candidates": [
                    "(A) 3",
                    "(B) 2",
                    "(C) 0",
                    "(D) 1"
                ],
                "answer": "(D) 1",
                "time_reference": "35:00-35:07",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many microphones are on stage during Donald Trump's speech?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) 1",
            "arguments": [
                "How many microphones are on stage during Donald Trump's speech?\nA. (A) 3\nB. (B) 2\nC. (C) 0\nD. (D) 1\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044c10>"
                    ]
                },
                83,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "86d8be9e5655e5366189a00c8d7d068c74c3621c130ced5d6c3b84d5849221cf",
            "target_hash": "130e553bd423b938999fb170663c3db89fe536737e0598ae8b8ff9079e800be9",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 84,
            "doc": {
                "candidates": [
                    "(A) Green",
                    "(B) Dark blue",
                    "(C) Bright red",
                    "(D) Yellow"
                ],
                "answer": "(D) Yellow",
                "time_reference": "35:35-35:50",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "question": "What color of coat does Melania Trump wear for the meeting that is hosted at Helsinki, Finland?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) Yellow",
            "arguments": [
                "What color of coat does Melania Trump wear for the meeting that is hosted at Helsinki, Finland?\nA. (A) Green\nB. (B) Dark blue\nC. (C) Bright red\nD. (D) Yellow\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0458d0>"
                    ]
                },
                84,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "65c05d5b5f6cb8a4955fa5e773290639bc28b5870c657ee58ae6fa752107bec1",
            "target_hash": "0b742116fd1f4311d53c40fdbfbf2f5ef15ee874a453ec11b4e98eb3b1a02775",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 85,
            "doc": {
                "candidates": [
                    "(A) Angry and expressionless",
                    "(B) Without any expression",
                    "(C) Laugh with excitement",
                    "(D) Sneaking and smiling"
                ],
                "answer": "(D) Sneaking and smiling",
                "time_reference": "38:30-38:37",
                "question_type": [
                    "event understanding"
                ],
                "question": "What was Putin's expression when he heard the speech Trump made?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) Sneaking and smiling",
            "arguments": [
                "What was Putin's expression when he heard the speech Trump made?\nA. (A) Angry and expressionless\nB. (B) Without any expression\nC. (C) Laugh with excitement\nD. (D) Sneaking and smiling\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0469b0>"
                    ]
                },
                85,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8d91cb7fd26459fbab402dc14760751373240ebdd6c019a2bc55ca428a424588",
            "target_hash": "5f665281c37546ef17681afe5df063fd58123deb5f9415e3ce59237e5b4ae6d6",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 86,
            "doc": {
                "candidates": [
                    "(A) 12",
                    "(B) 9",
                    "(C) 8",
                    "(D) 10"
                ],
                "answer": "(D) 10",
                "time_reference": "43:38-43:41",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "How many trucks are there in the garage with soldiers in white direct the way in the next scene after the Russian plane takes off?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(D) 10",
            "arguments": [
                "How many trucks are there in the garage with soldiers in white direct the way in the next scene after the Russian plane takes off?\nA. (A) 12\nB. (B) 9\nC. (C) 8\nD. (D) 10\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0474c0>"
                    ]
                },
                86,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c3e17ab1c0db5c71246ddebe432e43841d888770bb2448713eede2b4e69a71e2",
            "target_hash": "b3931203d1a663e06b612233d874ff901f5fe7ae7bb0614fbafa5decfa4ee2d3",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 87,
            "doc": {
                "candidates": [
                    "(A) 321",
                    "(B) 123",
                    "(C) 132",
                    "(D) 312"
                ],
                "answer": "(C) 132",
                "time_reference": "43:44-43:46",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is the number on the tank can be seen, with two tanks appear in front of it, in the snow?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(C) 132",
            "arguments": [
                "What is the number on the tank can be seen, with two tanks appear in front of it, in the snow?\nA. (A) 321\nB. (B) 123\nC. (C) 132\nD. (D) 312\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0477c0>"
                    ]
                },
                87,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5538db8ee303450b4f74b9bc9cd16b039126629329686d8cb75be5d5377e7df8",
            "target_hash": "b11e09441d2c35b8e4a70cca02f4080f988c284c3eb0576762e9c9d6ab1ab851",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 88,
            "doc": {
                "candidates": [
                    "(A) Communicate by letter",
                    "(B) Face-to-face communication",
                    "(C) Online video communication",
                    "(D) Communicate by telephone"
                ],
                "answer": "(C) Online video communication",
                "time_reference": "46:09-46:19",
                "question_type": [
                    "event understanding"
                ],
                "question": "After Yevgenia Albats spoke, how did Putin and Biden communicate in the next scene?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(C) Online video communication",
            "arguments": [
                "After Yevgenia Albats spoke, how did Putin and Biden communicate in the next scene?\nA. (A) Communicate by letter\nB. (B) Face-to-face communication\nC. (C) Online video communication\nD. (D) Communicate by telephone\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1d80>"
                    ]
                },
                88,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "08e73be05a60d55d567e81951c37ec2200b15f5de5fcf094c4ddfcee8f85e801",
            "target_hash": "f57986711322dc90c741bd4cf11b8165d6a77cf8e453781c8519bb411e5f000b",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 89,
            "doc": {
                "candidates": [
                    "(A) Russian Federation",
                    "(B) The Republic of Belarus",
                    "(C) Ukraine",
                    "(D) France"
                ],
                "answer": "(A) Russian Federation",
                "time_reference": "49:25-49:28",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Which country's flag is on the flagpole on the top of the building with gold clock?",
                "type": "documentary",
                "video": "aJI8XTa_DII"
            },
            "target": "(A) Russian Federation",
            "arguments": [
                "Which country's flag is on the flagpole on the top of the building with gold clock?\nA. (A) Russian Federation\nB. (B) The Republic of Belarus\nC. (C) Ukraine\nD. (D) France\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2c50>"
                    ]
                },
                89,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1abdc7aa4b359337e9891f69dcb525923e0e20b91195d1ac551a30e280510a74",
            "target_hash": "5970e7bd93216d8a5dd0aaf1e7b7c1fc73bebb66a3f3e00a84a8b8fd863357cf",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 90,
            "doc": {
                "candidates": [
                    "(A) Jane Mayer",
                    "(B) James Clapper",
                    "(C) Matt Apuzzo",
                    "(D) Donald John Trump"
                ],
                "answer": "(D) Donald John Trump",
                "time_reference": "06:46-06:46",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Who are the main individuals featured in this video?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(D) Donald John Trump",
            "arguments": [
                "Who are the main individuals featured in this video?\nA. (A) Jane Mayer\nB. (B) James Clapper\nC. (C) Matt Apuzzo\nD. (D) Donald John Trump\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3b50>"
                    ]
                },
                90,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5ef8416e1a51eca512743ca4c21c1409d9aa70d81ae829f8505f7a75d787ebb9",
            "target_hash": "c3c5da7dc207784cf97fb9f2c53dc41293f5e516abc3bf9971d273742368f353",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 91,
            "doc": {
                "candidates": [
                    "(A) A mirror",
                    "(B) An industrial robot",
                    "(C) A camera",
                    "(D) A car"
                ],
                "answer": "(D) A car",
                "time_reference": "24:20-24:20",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What object is in front of the J. Edgar Hoover FBI Building sign?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(D) A car",
            "arguments": [
                "What object is in front of the J. Edgar Hoover FBI Building sign?\nA. (A) A mirror\nB. (B) An industrial robot\nC. (C) A camera\nD. (D) A car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0446a0>"
                    ]
                },
                91,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1feee214670d80ef04fe4bc8ae5f8407730ec0a2ca1d1ad8b574ba9685f6e8c2",
            "target_hash": "a029d5e1ab73f1ce1ae401a5c30477ed96cfb7075dde94aabc55ac88753c7b8c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 92,
            "doc": {
                "candidates": [
                    "(A) He's preparing to shake hands",
                    "(B) It's a gesture of politeness, indicating that people can start moving around",
                    "(C) He's pointing the way for the audience",
                    "(D) He's waving to greet someone"
                ],
                "answer": "(B) It's a gesture of politeness, indicating that people can start moving around",
                "time_reference": "31:37-31:39",
                "question_type": [
                    "reasoning",
                    "key information retrieval"
                ],
                "question": "What does it mean when Trump raises his hand in front of a green helicopter with United States of America written on it?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(B) It's a gesture of politeness, indicating that people can start moving around",
            "arguments": [
                "What does it mean when Trump raises his hand in front of a green helicopter with United States of America written on it?\nA. (A) He's preparing to shake hands\nB. (B) It's a gesture of politeness, indicating that people can start moving around\nC. (C) He's pointing the way for the audience\nD. (D) He's waving to greet someone\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0445e0>"
                    ]
                },
                92,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8b50ee071f7febd1c7770dd9763f13640f8006ad212c72b0ddc4952a708b68c3",
            "target_hash": "8c2f14e91983fdb102f0e01081968d75af4e831f4dc8ef8d53105c2f8a8c8f4e",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 93,
            "doc": {
                "candidates": [
                    "(A) Trump team needs to treat Russia as a serious issue",
                    "(B) Meetings could fuel new calls for special counse",
                    "(C) Sessions Controversy Heightens Trump's Feeling of Being UnderSiege",
                    "(D) Why Trump's attorney general has come under fire for earlier contaects"
                ],
                "answer": "(A) Trump team needs to treat Russia as a serious issue",
                "time_reference": "41:04-41:04",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is the headline of the second news report?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(A) Trump team needs to treat Russia as a serious issue",
            "arguments": [
                "What is the headline of the second news report?\nA. (A) Trump team needs to treat Russia as a serious issue\nB. (B) Meetings could fuel new calls for special counse\nC. (C) Sessions Controversy Heightens Trump's Feeling of Being UnderSiege\nD. (D) Why Trump's attorney general has come under fire for earlier contaects\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046950>"
                    ]
                },
                93,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "29f75a7a6f38d718b9c1927a034f6cc5f10dab608a75c8d8be6ab6a794441ff3",
            "target_hash": "d4f1a42ed911d69717847e3b3cf31d7770d0e210d1d006598700dbf95a1a03df",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 94,
            "doc": {
                "candidates": [
                    "(A) He is looking at his left",
                    "(B) He is looking at his right",
                    "(C) He is looking at the ceiling",
                    "(D) He is looking at the floor"
                ],
                "answer": "(B) He is looking at his right",
                "time_reference": "49:34-49:40",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Which direction does Don Mcgahn look at in the black and white picture that has a portrait behind his back?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(B) He is looking at his right",
            "arguments": [
                "Which direction does Don Mcgahn look at in the black and white picture that has a portrait behind his back?\nA. (A) He is looking at his left\nB. (B) He is looking at his right\nC. (C) He is looking at the ceiling\nD. (D) He is looking at the floor\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047820>"
                    ]
                },
                94,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f7379be48d854dacc26b00260baef1d1de69b6be608e53e01849d9cc31f78164",
            "target_hash": "6f9ef438f603548128728e8ed090f67214bdf9441bdd5034e6619e2ccce1b332",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 95,
            "doc": {
                "candidates": [
                    "(A) To highlight the central theme of the documentary, serving as a thematic anchor",
                    "(B) To support a specific point",
                    "(C) As evidence presented in a court trial",
                    "(D) To visually emphasize the theme of the two documents"
                ],
                "answer": "(B) To support a specific point",
                "time_reference": "52:33-52:43",
                "question_type": [
                    "reasoning"
                ],
                "question": "Why two sentences have red underlines on two different documents?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(B) To support a specific point",
            "arguments": [
                "Why two sentences have red underlines on two different documents?\nA. (A) To highlight the central theme of the documentary, serving as a thematic anchor\nB. (B) To support a specific point\nC. (C) As evidence presented in a court trial\nD. (D) To visually emphasize the theme of the two documents\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922c730>"
                    ]
                },
                95,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "dd970b66a909c9c858ad26104327462374d388e32ed5d4009346bfb465bfe755",
            "target_hash": "11006d59f0a4de7b727bd0a000f2a532d383de66669435bbd4c49933f6d79fac",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 96,
            "doc": {
                "candidates": [
                    "(A) Donald Trump is seen engaging in a series of interviews with various hosts",
                    "(B) Trump holds a press conference, addressing recent developments and answering questions from journalists",
                    "(C) Trump issues a national statement, addressing the nation on a significant issue or event",
                    "(D) Trump is seen attending a court hearing, likely related to a legal matter concerning his administration or personal affairs"
                ],
                "answer": "(A) Donald Trump is seen engaging in a series of interviews with various hosts",
                "time_reference": "60:10-60:36",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "question": "What is the best summary for 60:10-60:36?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(A) Donald Trump is seen engaging in a series of interviews with various hosts",
            "arguments": [
                "What is the best summary for 60:10-60:36?\nA. (A) Donald Trump is seen engaging in a series of interviews with various hosts\nB. (B) Trump holds a press conference, addressing recent developments and answering questions from journalists\nC. (C) Trump issues a national statement, addressing the nation on a significant issue or event\nD. (D) Trump is seen attending a court hearing, likely related to a legal matter concerning his administration or personal affairs\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1780>"
                    ]
                },
                96,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "832475ab27fe4e21601c34aaaebfa5ce6af38857f32c16a4a9259ecd0e004f7a",
            "target_hash": "54c54abaf6166b9e6762af127fe6d5e540d047b27fa42ec80aa866d7643f5607",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 97,
            "doc": {
                "candidates": [
                    "(A) The location seems to be a shopping mall entrance",
                    "(B) The video is likely taken in a public transportation station",
                    "(C) The setting appears to be an airport terminal",
                    "(D) The video is shot at a parking lot exit gate"
                ],
                "answer": "(D) The video is shot at a parking lot exit gate",
                "time_reference": "83:44-83:51",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is the place that the footage shows a red car passes when the light is green, and a person wearing a white shirt with a red circle on her left sleeve, a black pant, standing in front of the light?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(D) The video is shot at a parking lot exit gate",
            "arguments": [
                "What is the place that the footage shows a red car passes when the light is green, and a person wearing a white shirt with a red circle on her left sleeve, a black pant, standing in front of the light?\nA. (A) The location seems to be a shopping mall entrance\nB. (B) The video is likely taken in a public transportation station\nC. (C) The setting appears to be an airport terminal\nD. (D) The video is shot at a parking lot exit gate\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2b60>"
                    ]
                },
                97,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bc6985f5549ce65c469105275bd12a4b4233d62b48e67d7e5560b79fc73bce6f",
            "target_hash": "bc71bcd82ff62ec1c79d8acc46932380df8bc6af734a67646454c223f02f478c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 98,
            "doc": {
                "candidates": [
                    "(A) A circle of white dotted patterns",
                    "(B) A circle of white chevron patterns",
                    "(C) A circle of white name tags",
                    "(D) A circle of white government documents"
                ],
                "answer": "(C) A circle of white name tags",
                "time_reference": "90:09-90:09",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is in the center of the table?",
                "type": "documentary",
                "video": "IsdbCjlZ5cQ"
            },
            "target": "(C) A circle of white name tags",
            "arguments": [
                "What is in the center of the table?\nA. (A) A circle of white dotted patterns\nB. (B) A circle of white chevron patterns\nC. (C) A circle of white name tags\nD. (D) A circle of white government documents\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3b80>"
                    ]
                },
                98,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "81548861aa037ba81b767b0e00cf6d9e5777f4df9294444bbeb7e579ee1edf6b",
            "target_hash": "59bfb9ad5845912092a1f29e2609c70247239aca14b14a561d0b646104cdeb43",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 99,
            "doc": {
                "candidates": [
                    "(A) From frowning to happy",
                    "(B) Keep frowning",
                    "(C) From happy to frowning",
                    "(D) From frowning to angry"
                ],
                "answer": "(A) From frowning to happy",
                "time_reference": "12:30-14:00",
                "question_type": [
                    "reasoning"
                ],
                "question": "How does the emotion change on Sarah's face when interviewed?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(A) From frowning to happy",
            "arguments": [
                "How does the emotion change on Sarah's face when interviewed?\nA. (A) From frowning to happy\nB. (B) Keep frowning\nC. (C) From happy to frowning\nD. (D) From frowning to angry\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044040>"
                    ]
                },
                99,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "065adde600493bcf6c41014d45b91354955b7f89a1444d0d2ef31817fc83e3db",
            "target_hash": "0250d18a8a145648a51d827cf89e443267482439dc38b1093339e77aba7ce022",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 100,
            "doc": {
                "candidates": [
                    "(A) Volatility and Anger",
                    "(B) Whatever It Takes",
                    "(C) Orgy of Speculation",
                    "(D) The Fed Blinked"
                ],
                "answer": "(A) Volatility and Anger",
                "time_reference": "16:00-16:00",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is the name of Chapter2?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(A) Volatility and Anger",
            "arguments": [
                "What is the name of Chapter2?\nA. (A) Volatility and Anger\nB. (B) Whatever It Takes\nC. (C) Orgy of Speculation\nD. (D) The Fed Blinked\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045b40>"
                    ]
                },
                100,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c7d26383214aa8d5d05b0a325f90d0a525fd23fbcdee1daeda1944d38eff26e1",
            "target_hash": "a6314dfd7ffd19dfad622636a57b7010df70f2ebf6737e05df393f457eafedb9",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 101,
            "doc": {
                "candidates": [
                    "(A) Thay are all men",
                    "(B) They are the elderly",
                    "(C) They wear red clothes",
                    "(D) They are tall"
                ],
                "answer": "(C) They wear red clothes",
                "time_reference": "18:00-18:53",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What characteristic does the Tea Party have in common in this people?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(C) They wear red clothes",
            "arguments": [
                "What characteristic does the Tea Party have in common in this people?\nA. (A) Thay are all men\nB. (B) They are the elderly\nC. (C) They wear red clothes\nD. (D) They are tall\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045c30>"
                    ]
                },
                101,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5e069b6b52f08ffa291f154ddee311a16780754d8219333523d5c6b30fb05fbd",
            "target_hash": "9f12d06d8cae236c4d41d0ea27759b076c2db2ac1567dcdd047a9de20b15b388",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 102,
            "doc": {
                "candidates": [
                    "(A) In his office",
                    "(B) In a garden",
                    "(C) In a beach",
                    "(D) In his home"
                ],
                "answer": "(D) In his home",
                "time_reference": "51:00-52:00",
                "question_type": [
                    "event understanding"
                ],
                "question": "What place would Mohamed's interview most likely to take place?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(D) In his home",
            "arguments": [
                "What place would Mohamed's interview most likely to take place?\nA. (A) In his office\nB. (B) In a garden\nC. (C) In a beach\nD. (D) In his home\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047730>"
                    ]
                },
                102,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "94163280241ec45ea16e5c2c3d5706ae26f5864ddd69d42afc6fe38a52bacb22",
            "target_hash": "81e2071931e9e122da8ab4b200a288f6cd448641663df342e990ec76fc133148",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 103,
            "doc": {
                "candidates": [
                    "(A) Pigeon and crow",
                    "(B) Dog and pigeon",
                    "(C) Pigeon and bull",
                    "(D) Crow and bull"
                ],
                "answer": "(A) Pigeon and crow",
                "time_reference": "61:45-61:45",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What animal is on the street next to the famous Wall Street statue?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(A) Pigeon and crow",
            "arguments": [
                "What animal is on the street next to the famous Wall Street statue?\nA. (A) Pigeon and crow\nB. (B) Dog and pigeon\nC. (C) Pigeon and bull\nD. (D) Crow and bull\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922c640>"
                    ]
                },
                103,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9e1a34e98ee93b69f8f4b62e11169b605a4620fee5217a26b46e27b988572a57",
            "target_hash": "f65e37aee53402e64251541a9fae333e0fafa4704e52e5b24ca1102c8c176b5b",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 104,
            "doc": {
                "candidates": [
                    "(A) Cigarette",
                    "(B) Phone",
                    "(C) Food",
                    "(D) Pen"
                ],
                "answer": "(D) Pen",
                "time_reference": "62:18-62:46",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What does Jacoby have on his hand in the interview with Jeremy?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(D) Pen",
            "arguments": [
                "What does Jacoby have on his hand in the interview with Jeremy?\nA. (A) Cigarette\nB. (B) Phone\nC. (C) Food\nD. (D) Pen\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1db0>"
                    ]
                },
                104,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "76bbd41112d6f9eeeb83cb9f4d430065504fd86876697a81db31d63dc87a201f",
            "target_hash": "a415e5964f08f8bd72862780658745a0dd97ba191da681dadbd5ad3e221f8818",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 105,
            "doc": {
                "candidates": [
                    "(A) +51.28, 1.86%",
                    "(B) +644.10, 1.44%",
                    "(C) +71.88, 0.89%",
                    "(D) +164.93, 2.90%"
                ],
                "answer": "(C) +71.88, 0.89%",
                "time_reference": "70:35-70:35",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What are the changing trends of NASDAQ when the changing trends of Dow Jones are +457.21, 1.95%?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(C) +71.88, 0.89%",
            "arguments": [
                "What are the changing trends of NASDAQ when the changing trends of Dow Jones are +457.21, 1.95%?\nA. (A) +51.28, 1.86%\nB. (B) +644.10, 1.44%\nC. (C) +71.88, 0.89%\nD. (D) +164.93, 2.90%\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1960>"
                    ]
                },
                105,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0e33d6f11ab52373c0557ce466c22f774ddcb709aec7ff2f564321671a33f45c",
            "target_hash": "50d6681fd9c1e2c9afea6e278f9814da193baae7a12d993c350b31ef0e4d0654",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 106,
            "doc": {
                "candidates": [
                    "(A) Yellow",
                    "(B) White",
                    "(C) Black",
                    "(D) Blue"
                ],
                "answer": "(D) Blue",
                "time_reference": "84:13-84:13",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What colour is Tom Kertis's clothes?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(D) Blue",
            "arguments": [
                "What colour is Tom Kertis's clothes?\nA. (A) Yellow\nB. (B) White\nC. (C) Black\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd21d0>"
                    ]
                },
                106,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "64b64b9309d2b0b1221cdc3351f9ae27b71d50bf6631bfb5dace5365f6a206e1",
            "target_hash": "2af8475f8d12112df3b1487762f27a4f3153636b07666dd9be3936b9d3a38176",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 107,
            "doc": {
                "candidates": [
                    "(A) Prisident",
                    "(B) Unemployed",
                    "(C) Economist",
                    "(D) Worker"
                ],
                "answer": "(C) Economist",
                "time_reference": "00:00-113:00",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the most common job for the interviewees?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(C) Economist",
            "arguments": [
                "What is the most common job for the interviewees?\nA. (A) Prisident\nB. (B) Unemployed\nC. (C) Economist\nD. (D) Worker\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044e50>"
                    ]
                },
                107,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c32cad4cf55d18f6d56ecded86943da2ef40c6ce0318a3590edf52a3a50b0fb8",
            "target_hash": "5d2cd4bd5d76ebd23f234200407875e72296aba438787740deb1e5157e374eef",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 108,
            "doc": {
                "candidates": [
                    "(A) A documentary about America",
                    "(B) A documentary about the economy",
                    "(C) A documentary about the sport",
                    "(D) A documentary about the festival"
                ],
                "answer": "(B) A documentary about the economy",
                "time_reference": "00:00-113:00",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is the category of this video?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(B) A documentary about the economy",
            "arguments": [
                "What is the category of this video?\nA. (A) A documentary about America\nB. (B) A documentary about the economy\nC. (C) A documentary about the sport\nD. (D) A documentary about the festival\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045cf0>"
                    ]
                },
                108,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1d8f1f164b6c1a883d384db20d740e63a566d448dc5a5a115e5c91c1006bb683",
            "target_hash": "40ef6c9885f5b743df00d67adb91a47376c68e721e9225db661ff29451a0f99f",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 109,
            "doc": {
                "candidates": [
                    "(A) Dress",
                    "(B) Suit",
                    "(C) Sportswear",
                    "(D) Swimwear"
                ],
                "answer": "(B) Suit",
                "time_reference": "00:00-113:00",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the most common costume in the video?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(B) Suit",
            "arguments": [
                "What is the most common costume in the video?\nA. (A) Dress\nB. (B) Suit\nC. (C) Sportswear\nD. (D) Swimwear\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0468f0>"
                    ]
                },
                109,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4c6072409c5eee515d14e14eb96b5917627f9ceef1fc7aabcf78ab35c0e20c4f",
            "target_hash": "c485ccf7260c2ce1fcb6e4f9ebdc6ad6b891c71e5b088ae55c920cafc7c9ec24",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 110,
            "doc": {
                "candidates": [
                    "(A) Money",
                    "(B) Cars",
                    "(C) Skyscraper",
                    "(D) Trees"
                ],
                "answer": "(C) Skyscraper",
                "time_reference": "00:00-113:00",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the most frequent element in the video?",
                "type": "documentary",
                "video": "EpMLAQbSYAw"
            },
            "target": "(C) Skyscraper",
            "arguments": [
                "What is the most frequent element in the video?\nA. (A) Money\nB. (B) Cars\nC. (C) Skyscraper\nD. (D) Trees\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0478b0>"
                    ]
                },
                110,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "01fdba18c0968b53b84c67d5a5459177372614cad8246450d208e9fea976c11e",
            "target_hash": "c670b97ecf0d91146e118206c917ed4260ad9ae126b3d238f79b06c58ad2056e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 111,
            "doc": {
                "candidates": [
                    "(A) Because eating noodles is not allowed inside the 711 convenience store",
                    "(B) Because filming is not allowed inside the 711 convenience store",
                    "(C) Because they buy ramen from a convenience store, and there are no seats inside",
                    "(D) Because they are waiting for a bus at the bus stop"
                ],
                "answer": "(C) Because they buy ramen from a convenience store, and there are no seats inside",
                "time_reference": "03:56-05:00",
                "question_type": [
                    "reasoning"
                ],
                "question": "What is the possible reason why two hosts are standing on the roadside with bowls in their hands?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(C) Because they buy ramen from a convenience store, and there are no seats inside",
            "arguments": [
                "What is the possible reason why two hosts are standing on the roadside with bowls in their hands?\nA. (A) Because eating noodles is not allowed inside the 711 convenience store\nB. (B) Because filming is not allowed inside the 711 convenience store\nC. (C) Because they buy ramen from a convenience store, and there are no seats inside\nD. (D) Because they are waiting for a bus at the bus stop\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922c7c0>"
                    ]
                },
                111,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e157ee21f4cf50979927c9fa523f229f42e834483ad2f21651b622ca86a8d8cb",
            "target_hash": "3e9c751bdb128bb2bf19d7d4244a9a10af11b0446c8816e756a17732f7e05fa3",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 112,
            "doc": {
                "candidates": [
                    "(A) $10",
                    "(B) 100",
                    "(C) $100",
                    "(D) 100 Japanese Yen"
                ],
                "answer": "(C) $100",
                "time_reference": "14:16-14:16",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "How much does a ramen set cost in the second ramen shop?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(C) $100",
            "arguments": [
                "How much does a ramen set cost in the second ramen shop?\nA. (A) $10\nB. (B) 100\nC. (C) $100\nD. (D) 100 Japanese Yen\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd1f60>"
                    ]
                },
                112,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "718f3e93aa8ecb7f86561fa1d13c935d29a1ade82045a95c09e5162df6dfa8ff",
            "target_hash": "8b1f568e8d2a5a3b0c740e4b4ebf2d57c61a4338cd55a6bf068b90cf462e5e66",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 113,
            "doc": {
                "candidates": [
                    "(A) The owner",
                    "(B) A food critic",
                    "(C) A special guest comedian",
                    "(D) A waiter"
                ],
                "answer": "(A) The owner",
                "time_reference": "21:23-21:23",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is Keita Karino's identity in the store?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(A) The owner",
            "arguments": [
                "What is Keita Karino's identity in the store?\nA. (A) The owner\nB. (B) A food critic\nC. (C) A special guest comedian\nD. (D) A waiter\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2f80>"
                    ]
                },
                113,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1332a5185ce500b11e463f53136b26ad4ddeb0f403d7b5b410b7596e76b34b71",
            "target_hash": "77d62be806cf7a136fff0d6b5b5873fa06905580127a94c4e61ea8af99746f1d",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 114,
            "doc": {
                "candidates": [
                    "(A) 8 days ago",
                    "(B) 8 months ago",
                    "(C) 4 months ago",
                    "(D) 3 months ago"
                ],
                "answer": "(B) 8 months ago",
                "time_reference": "27:17-27:17",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "When does Oliver_B post his comment?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(B) 8 months ago",
            "arguments": [
                "When does Oliver_B post his comment?\nA. (A) 8 days ago\nB. (B) 8 months ago\nC. (C) 4 months ago\nD. (D) 3 months ago\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3f40>"
                    ]
                },
                114,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4d85de8c4511fd2cfb9fb5cc9deb960aec1225cbdd7297a8829deb5e505bc704",
            "target_hash": "1cd43fc04d85c9dffd2ed9694de181127aafe1d20947f42ed13075e0c8a9542d",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 115,
            "doc": {
                "candidates": [
                    "(A) Natto",
                    "(B) Rice",
                    "(C) Soybeans",
                    "(D) Grasshoppers"
                ],
                "answer": "(D) Grasshoppers",
                "time_reference": "28:06-33:00",
                "question_type": [
                    "event understanding"
                ],
                "question": "What are the two hosts looking for in the field?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(D) Grasshoppers",
            "arguments": [
                "What are the two hosts looking for in the field?\nA. (A) Natto\nB. (B) Rice\nC. (C) Soybeans\nD. (D) Grasshoppers\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044f70>"
                    ]
                },
                115,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3d78cd912942fcb14135b02c2a062790892fff2a856a27db88af65d0e49a11ec",
            "target_hash": "869cb012d776146bbde30c0b3bce63756e4b6a91a33f8ebc1b12796f39d3e9d2",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 116,
            "doc": {
                "candidates": [
                    "(A) She is very sad",
                    "(B) She is very happy and can't wait to taste the dish",
                    "(C) She is angry and feels disgusted",
                    "(D) She frowns and appears puzzled"
                ],
                "answer": "(D) She frowns and appears puzzled",
                "time_reference": "40:20-40:20",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "question": "How does the female host express herself at 40:20?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(D) She frowns and appears puzzled",
            "arguments": [
                "How does the female host express herself at 40:20?\nA. (A) She is very sad\nB. (B) She is very happy and can't wait to taste the dish\nC. (C) She is angry and feels disgusted\nD. (D) She frowns and appears puzzled\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045e10>"
                    ]
                },
                116,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5541e074a7a848005b9aef2157c8c324edea10ae112b175c9cc0e57ac7264c7d",
            "target_hash": "2942c111a74bd61953a8285ac0ca81f5207680084fbc367a7a9c57fd22997f4c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 117,
            "doc": {
                "candidates": [
                    "(A) Drying them",
                    "(B) Cleaning them",
                    "(C) Cooking them",
                    "(D) Mincing them"
                ],
                "answer": "(C) Cooking them",
                "time_reference": "43:15-43:20",
                "question_type": [
                    "event understanding"
                ],
                "question": "Why does the worker pour the small fish from the blue basket into the large container?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(C) Cooking them",
            "arguments": [
                "Why does the worker pour the small fish from the blue basket into the large container?\nA. (A) Drying them\nB. (B) Cleaning them\nC. (C) Cooking them\nD. (D) Mincing them\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046aa0>"
                    ]
                },
                117,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9e3e384150aceebd7752c29d5ba5e7ec73b09209129fe565f8ccda8aecb79ee0",
            "target_hash": "1154f5fe1c9af48a65e477d28626ba355a10f924965f9903e9cf21bbed8e74ab",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 118,
            "doc": {
                "candidates": [
                    "(A) The grading process of Wagyu beef",
                    "(B) The process of raising Wagyu beef",
                    "(C) The transportation of Wagyu beef to restaurants",
                    "(D) The storage of Wagyu beef in a factory"
                ],
                "answer": "(A) The grading process of Wagyu beef",
                "time_reference": "68:09-71:16",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "question": "What is the main content introduced in the video segment from 68:09-71:16?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(A) The grading process of Wagyu beef",
            "arguments": [
                "What is the main content introduced in the video segment from 68:09-71:16?\nA. (A) The grading process of Wagyu beef\nB. (B) The process of raising Wagyu beef\nC. (C) The transportation of Wagyu beef to restaurants\nD. (D) The storage of Wagyu beef in a factory\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047a00>"
                    ]
                },
                118,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0d3aaa083d953517031ce6bb2c4e94d7fb0930eb569f0df5d9611bf819b37bb0",
            "target_hash": "190d7099111f5b285a7fe1697903f61d03c488fa1cd1d9a31f9e3e5efdfc2376",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 119,
            "doc": {
                "candidates": [
                    "(A) In the soup",
                    "(B) Inside pork dumplings",
                    "(C) In the rice",
                    "(D) Inside fish meat dumplings"
                ],
                "answer": "(D) Inside fish meat dumplings",
                "time_reference": "85:50-85:35",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "question": "Where is the fish head placed from 85:35-85:50?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(D) Inside fish meat dumplings",
            "arguments": [
                "Where is the fish head placed from 85:35-85:50?\nA. (A) In the soup\nB. (B) Inside pork dumplings\nC. (C) In the rice\nD. (D) Inside fish meat dumplings\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922c910>"
                    ]
                },
                119,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a2b0b112293b71918320c4b466a2937298beea453b485280f5de50fda7027073",
            "target_hash": "e6352ce20ae9461edfe8281c811bf40de966bb75f391cd7604132a8227efec25",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 120,
            "doc": {
                "candidates": [
                    "(A) Tokyo",
                    "(B) Sendai",
                    "(C) Fukushima",
                    "(D) Fukuoka"
                ],
                "answer": "(D) Fukuoka",
                "time_reference": "88:07-88:07",
                "question_type": [
                    "key information retrieval",
                    "temporal grounding"
                ],
                "question": "What text appears at 88:07?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(D) Fukuoka",
            "arguments": [
                "What text appears at 88:07?\nA. (A) Tokyo\nB. (B) Sendai\nC. (C) Fukushima\nD. (D) Fukuoka\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2080>"
                    ]
                },
                120,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3f870909297ffcb5adc80e2ad1593996364cc7f450d4a65f4229e17a6eb7f142",
            "target_hash": "a0b195f766b3b11086637757f75e075ff9b8760c26b6160ef6ad6d4fbcca59dd",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 121,
            "doc": {
                "candidates": [
                    "(A) From hopeful to seasick",
                    "(B) From excited to disappointed",
                    "(C) From relaxed to anxious",
                    "(D) From curious to bored"
                ],
                "answer": "(A) From hopeful to seasick",
                "time_reference": "53:26-54:16",
                "question_type": [
                    "reasoning"
                ],
                "question": "How does the male host feel on the ship?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(A) From hopeful to seasick",
            "arguments": [
                "How does the male host feel on the ship?\nA. (A) From hopeful to seasick\nB. (B) From excited to disappointed\nC. (C) From relaxed to anxious\nD. (D) From curious to bored\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd30a0>"
                    ]
                },
                121,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "67c3539403a8c2f4a5f235ac1d857f6085b0aa2c8ba8871a55ed2cc53349aad2",
            "target_hash": "658a1db71d8dc5c4f33652396680d9a8662d96e9aa41c5f7684b30a776e24580",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 122,
            "doc": {
                "candidates": [
                    "(A) Beef",
                    "(B) Deer meat",
                    "(C) Chicken",
                    "(D) Horse meat"
                ],
                "answer": "(B) Deer meat",
                "time_reference": "00:00-100:54",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is the last type of raw meat introduced in the video?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(B) Deer meat",
            "arguments": [
                "What is the last type of raw meat introduced in the video?\nA. (A) Beef\nB. (B) Deer meat\nC. (C) Chicken\nD. (D) Horse meat\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3310>"
                    ]
                },
                122,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "262619edcdc7c7bba8e63d36d9f2cb316c8336af49c3415ea4497335c11b4bd4",
            "target_hash": "fea12f5829fe472c95128a72e23c1025267517993a34d311fcdbb4cf50b92a77",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 123,
            "doc": {
                "candidates": [
                    "(A) Alipay",
                    "(B) RPay",
                    "(C) Apple Pay",
                    "(D) WeChat Pay"
                ],
                "answer": "(B) RPay",
                "time_reference": "99:35-99:35",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What payment method appears in the video at last?",
                "type": "documentary",
                "video": "KktLi3UifPY"
            },
            "target": "(B) RPay",
            "arguments": [
                "What payment method appears in the video at last?\nA. (A) Alipay\nB. (B) RPay\nC. (C) Apple Pay\nD. (D) WeChat Pay\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045090>"
                    ]
                },
                123,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f6c1b884cfc44bddc5d25b4a9172511699b636798635c7df39a1aec9641b06c1",
            "target_hash": "3b9d38b65adf500e198b62271408c97d0e0e767f0628ef719e5932399d8288a0",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 124,
            "doc": {
                "candidates": [
                    "(A) Orange juice",
                    "(B) Homemade Masala",
                    "(C) Lemon juice",
                    "(D) Pepper water"
                ],
                "answer": "(B) Homemade Masala",
                "time_reference": "01:09-01:15",
                "question_type": [
                    "event understanding"
                ],
                "question": "What liquid is added on top of the grilled chicken at the beginning of the video?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(B) Homemade Masala",
            "arguments": [
                "What liquid is added on top of the grilled chicken at the beginning of the video?\nA. (A) Orange juice\nB. (B) Homemade Masala\nC. (C) Lemon juice\nD. (D) Pepper water\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045f30>"
                    ]
                },
                124,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b7f5a99eeb61621c4b5359da0c97eeb590798c563b5a45e1766910fb3eec8bc5",
            "target_hash": "011603014badf4f69aea01031648402405a9075ee9191c5b75529244d0e6c40f",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 125,
            "doc": {
                "candidates": [
                    "(A) Blue",
                    "(B) Orange",
                    "(C) White",
                    "(D) Black"
                ],
                "answer": "(B) Orange",
                "time_reference": "07:38-07:38",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "question": "What color are the chairs in the restaurant where the food travelers eat Peshawari chicken with roti?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(B) Orange",
            "arguments": [
                "What color are the chairs in the restaurant where the food travelers eat Peshawari chicken with roti?\nA. (A) Blue\nB. (B) Orange\nC. (C) White\nD. (D) Black\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046c50>"
                    ]
                },
                125,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6bdb77450b2c88fbfb531e7821c62190d08a937c5d70fbff697b629156786a2a",
            "target_hash": "1d5e0e9af1a6546cb8118be7370935727ec3fc75f5a4a1739b481b79605c418d",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 126,
            "doc": {
                "candidates": [
                    "(A) A duckbill cap",
                    "(B) A top hat",
                    "(C) A headscarf",
                    "(D) He isn't wearing anything"
                ],
                "answer": "(A) A duckbill cap",
                "time_reference": "09:55-20:50",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What decoration is the man wearing on his head while sitting at the table and talking with the American man in the video, but not eating together?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(A) A duckbill cap",
            "arguments": [
                "What decoration is the man wearing on his head while sitting at the table and talking with the American man in the video, but not eating together?\nA. (A) A duckbill cap\nB. (B) A top hat\nC. (C) A headscarf\nD. (D) He isn't wearing anything\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047b20>"
                    ]
                },
                126,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a9e63c1ace0a82cbb0d82ad8d6ba8f0f514203963aefd883e7bfbc85681f635e",
            "target_hash": "627836e9e8ce07cc7fb3e644d502955dffb4f51cdc9d7feeb50095da71cdcea7",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 127,
            "doc": {
                "candidates": [
                    "(A) 8",
                    "(B) 7",
                    "(C) 4",
                    "(D) 6"
                ],
                "answer": "(B) 7",
                "time_reference": "13:17-73:18",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many women does the American man have dinner face-to-face with in the video?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(B) 7",
            "arguments": [
                "How many women does the American man have dinner face-to-face with in the video?\nA. (A) 8\nB. (B) 7\nC. (C) 4\nD. (D) 6\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922ca30>"
                    ]
                },
                127,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2e70f3c3680964574e7407b97a4440a8eab516a71ee9082aaed21654e9c6db2d",
            "target_hash": "31fdd1beedc916020bd5e4594805b0f8033d1061246b9e1a1d07c1aa44ef9588",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 128,
            "doc": {
                "candidates": [
                    "(A) Forks",
                    "(B) Spoons",
                    "(C) Chopsticks",
                    "(D) Hands"
                ],
                "answer": "(D) Hands",
                "time_reference": "01:58-84:27",
                "question_type": [
                    "entity recognition"
                ],
                "question": "In this video, what is the primary method of eating?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(D) Hands",
            "arguments": [
                "In this video, what is the primary method of eating?\nA. (A) Forks\nB. (B) Spoons\nC. (C) Chopsticks\nD. (D) Hands\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd21a0>"
                    ]
                },
                128,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "53ce7ce936af9c7ced709c5ed240ce70a01d4468780c94b218e875350b184b75",
            "target_hash": "d16a0374c9cc7d086585ced790b085c151c01a814c9ad6aa1786ac7253e8efa0",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 129,
            "doc": {
                "candidates": [
                    "(A) Right",
                    "(B) Both",
                    "(C) Neither",
                    "(D) Left"
                ],
                "answer": "(A) Right",
                "time_reference": "22:10-22:30",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Which foot does the person in the video use to cut open the cow's stomach?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(A) Right",
            "arguments": [
                "Which foot does the person in the video use to cut open the cow's stomach?\nA. (A) Right\nB. (B) Both\nC. (C) Neither\nD. (D) Left\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd31c0>"
                    ]
                },
                129,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "28ac1009f283884db11c56bac86672fabf32466f8083b45cc13cf3b33fa2b10c",
            "target_hash": "f1a02424e7457a4fecdeb04b1595eab44a1128281ae2135230dd5a92e5704e1a",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 130,
            "doc": {
                "candidates": [
                    "(A) 2",
                    "(B) 6",
                    "(C) 8",
                    "(D) 4"
                ],
                "answer": "(C) 8",
                "time_reference": "34:14-36:27",
                "question_type": [
                    "entity recognition"
                ],
                "question": "In the video, how many people are having breakfast with the American man? (Excluding himself)",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(C) 8",
            "arguments": [
                "In the video, how many people are having breakfast with the American man? (Excluding himself)\nA. (A) 2\nB. (B) 6\nC. (C) 8\nD. (D) 4\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3550>"
                    ]
                },
                130,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3a7ed1c40f684d70c8427c32bef49620a135a7b407e4edf15180f2953ab74195",
            "target_hash": "bd726fe4af424e6905afb3056d7c1cecc71eedb4ef7be1fe382f0fa6d8493970",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 131,
            "doc": {
                "candidates": [
                    "(A) Drinking water",
                    "(B) Taking a breathe of cool air",
                    "(C) Dancing",
                    "(D) Eating orange"
                ],
                "answer": "(B) Taking a breathe of cool air",
                "time_reference": "44:08-44:11",
                "question_type": [
                    "event understanding"
                ],
                "question": "What does the food traveller do after eating the very spicy food?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(B) Taking a breathe of cool air",
            "arguments": [
                "What does the food traveller do after eating the very spicy food?\nA. (A) Drinking water\nB. (B) Taking a breathe of cool air\nC. (C) Dancing\nD. (D) Eating orange\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0451b0>"
                    ]
                },
                131,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1437690d5def4305e2358fed4849bc81d6c7e68c14dfe5b8c0641a4a450dc00d",
            "target_hash": "9efbd3efffcbedfd97508c1e763388c165d4c5e519661d3588fe5956372a69f0",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 132,
            "doc": {
                "candidates": [
                    "(A) 10",
                    "(B) 1",
                    "(C) 5",
                    "(D) 2"
                ],
                "answer": "(C) 5",
                "time_reference": "01:00-84:27",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many different outfits has the American man worn in total in the video?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(C) 5",
            "arguments": [
                "How many different outfits has the American man worn in total in the video?\nA. (A) 10\nB. (B) 1\nC. (C) 5\nD. (D) 2\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045870>"
                    ]
                },
                132,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4151e7a10eca9dd30fc8e1eaeb658b2d512f948878b4989f15c53ac6448dac2b",
            "target_hash": "7ec5055a12d4c160c0b5a3ffe1114a5b4db86c4e3a865a6d6e80665b19e83ad5",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 133,
            "doc": {
                "candidates": [
                    "(A) The sun setting behind the snowy mountains",
                    "(B) The fish swimming",
                    "(C) The children crying",
                    "(D) The sprout becomes a tree"
                ],
                "answer": "(A) The sun setting behind the snowy mountains",
                "time_reference": "63:30-63:30",
                "question_type": [
                    "event understanding"
                ],
                "question": "What scene appears while the American is chewing the corn-wheat bread?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(A) The sun setting behind the snowy mountains",
            "arguments": [
                "What scene appears while the American is chewing the corn-wheat bread?\nA. (A) The sun setting behind the snowy mountains\nB. (B) The fish swimming\nC. (C) The children crying\nD. (D) The sprout becomes a tree\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046d70>"
                    ]
                },
                133,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1c0a3320dcc38075ea4da9e83c3414ec9ebdee54ec9434f87ad45bd2a8bdc071",
            "target_hash": "88d73eaead54e1468f9ce15de2a19b5e1e70e506d171f1510cce16dba1d988c5",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 134,
            "doc": {
                "candidates": [
                    "(A) Counterclockwise",
                    "(B) Both ways",
                    "(C) There's no rotation",
                    "(D) Clockwise"
                ],
                "answer": "(A) Counterclockwise",
                "time_reference": "64:59-65:01",
                "question_type": [
                    "event understanding"
                ],
                "question": "Is the lady in the video stirring the water in the pot clockwise or counterclockwise when making the onion garlic noodle soup?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(A) Counterclockwise",
            "arguments": [
                "Is the lady in the video stirring the water in the pot clockwise or counterclockwise when making the onion garlic noodle soup?\nA. (A) Counterclockwise\nB. (B) Both ways\nC. (C) There's no rotation\nD. (D) Clockwise\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047c40>"
                    ]
                },
                134,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c26f86f4325279cb494ef177d949f228d67a03ec9f767a9129d0ddabeadc3005",
            "target_hash": "d287f7453f750854d093b7aae8bf101c2cc0256c61c5df61198f519780668a2c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 135,
            "doc": {
                "candidates": [
                    "(A) Dog",
                    "(B) Panda",
                    "(C) Sheep",
                    "(D) Yak"
                ],
                "answer": "(D) Yak",
                "time_reference": "69:06-69:06",
                "question_type": [
                    "reasoning",
                    "entity recognition",
                    "key information retrieval"
                ],
                "question": "Which animal does Kareem take care of?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(D) Yak",
            "arguments": [
                "Which animal does Kareem take care of?\nA. (A) Dog\nB. (B) Panda\nC. (C) Sheep\nD. (D) Yak\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922cb50>"
                    ]
                },
                135,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "46fb0d349533bf940aa1e2875fadc8bd7f188610549738bc204c6495e1720eb6",
            "target_hash": "a80d209d6a42e162c66051db0e6c305b3853c2ca62788668c97e7eba7e6633ed",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 136,
            "doc": {
                "candidates": [
                    "(A) They are trying to lasso the dog",
                    "(B) They are trying to lasso the cat",
                    "(C) They are trying to lasso the Panda",
                    "(D) They are trying to lasso the yak"
                ],
                "answer": "(D) They are trying to lasso the yak",
                "time_reference": "69:40-69:48",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What are the people holding the yellow ropes trying to do at the place that has trees with blooms?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(D) They are trying to lasso the yak",
            "arguments": [
                "What are the people holding the yellow ropes trying to do at the place that has trees with blooms?\nA. (A) They are trying to lasso the dog\nB. (B) They are trying to lasso the cat\nC. (C) They are trying to lasso the Panda\nD. (D) They are trying to lasso the yak\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd22c0>"
                    ]
                },
                136,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d25a16d16c3fbeab29921b2276e6e5132481237a6091e7a1e00887a76443f304",
            "target_hash": "2a9b9b29920234b358ca10d1484b78ed62960ce3fe226013557b8ada28229682",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 137,
            "doc": {
                "candidates": [
                    "(A) The women and children are eating together",
                    "(B) The men and women are eating together",
                    "(C) The men and children are eating together",
                    "(D) They are all eating separately"
                ],
                "answer": "(A) The women and children are eating together",
                "time_reference": "81:30-84:21",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Are the men, women, and children in the valley eating together in the video?",
                "type": "documentary",
                "video": "wNCPgIVz15c"
            },
            "target": "(A) The women and children are eating together",
            "arguments": [
                "Are the men, women, and children in the valley eating together in the video?\nA. (A) The women and children are eating together\nB. (B) The men and women are eating together\nC. (C) The men and children are eating together\nD. (D) They are all eating separately\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd32e0>"
                    ]
                },
                137,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a8213016e57b4e372d14c7b3d41c927d888c4a38b26cf834ab9ecf28c4d201e1",
            "target_hash": "25fd5d753e833e65d63644e8be7ca6b050f324fe4f9420158c0a5759c517abb4",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 138,
            "doc": {
                "candidates": [
                    "(A) The photographer has recorded the magnificent marine landscape. While seated in a pneumatic boat and diving, he captured scenes of pods of sperm whales active in the area",
                    "(B) The photographer captured scenes of pods of orca active in the marine area from aboard a cruise ship",
                    "(C) Many sperm whales are active in this sea region, and the photographer, seated in a helicopter, pursues them to film and document their behavior",
                    "(D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior"
                ],
                "answer": "(D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior",
                "time_reference": "01:17-05:07",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "question": "What are the key events or actions that occur from 01:17 - 05:07?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior",
            "arguments": [
                "What are the key events or actions that occur from 01:17 - 05:07?\nA. (A) The photographer has recorded the magnificent marine landscape. While seated in a pneumatic boat and diving, he captured scenes of pods of sperm whales active in the area\nB. (B) The photographer captured scenes of pods of orca active in the marine area from aboard a cruise ship\nC. (C) Many sperm whales are active in this sea region, and the photographer, seated in a helicopter, pursues them to film and document their behavior\nD. (D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044430>"
                    ]
                },
                138,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b86fb7d3909e04224f33e487eb717bd036bc5672bcb29191a2e8a5392e958baf",
            "target_hash": "bc98ee0b6b59360c32badbb0b2ffdab4cd0277cc8b73b2342e7b5ed7ecf02296",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 139,
            "doc": {
                "candidates": [
                    "(A) Transported to the destination in a box via cruise ship",
                    "(B) Transported to the island in a box via helicopter",
                    "(C) Transported to the island from an inflatable rubber boat",
                    "(D) Transported to the destination from the mainland by pickup truck"
                ],
                "answer": "(B) Transported to the island in a box via helicopter",
                "time_reference": "05:42-06:30",
                "question_type": [
                    "reasoning"
                ],
                "question": "How are the photographer's equipment transported to the island?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(B) Transported to the island in a box via helicopter",
            "arguments": [
                "How are the photographer's equipment transported to the island?\nA. (A) Transported to the destination in a box via cruise ship\nB. (B) Transported to the island in a box via helicopter\nC. (C) Transported to the island from an inflatable rubber boat\nD. (D) Transported to the destination from the mainland by pickup truck\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0452d0>"
                    ]
                },
                139,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9160ff2e8b609cd566498a80cb43dd6689e7b7f524efda52e771f3b81f5198ec",
            "target_hash": "797a2f3298abeedafed85d354bb27d091d5fa62c36b2a590cc23c3cb69e9a958",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 140,
            "doc": {
                "candidates": [
                    "(A) Arctic expedition documentary",
                    "(B) Film on human-ocean themes",
                    "(C) Marine environment conservation documentary",
                    "(D) Wildlife documentary"
                ],
                "answer": "(D) Wildlife documentary",
                "time_reference": "08:48-11:16",
                "question_type": [
                    "event understanding"
                ],
                "question": "What category or genre does this video clip belong to?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(D) Wildlife documentary",
            "arguments": [
                "What category or genre does this video clip belong to?\nA. (A) Arctic expedition documentary\nB. (B) Film on human-ocean themes\nC. (C) Marine environment conservation documentary\nD. (D) Wildlife documentary\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044af0>"
                    ]
                },
                140,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "7cff7ef193681bc7706f0f31d3430bdc61c59ac8fee2233f17ac72f3e86857a7",
            "target_hash": "dd6aceb3504ac36046f7608fd97d67606cbcf49fdd6348e7b9481afcebdc3df2",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 141,
            "doc": {
                "candidates": [
                    "(A) This person witnessed people hunting orca on the beach, so the person was angry",
                    "(B) This person saw an orca hunting elephant seals on the beach, so he attempted to drive away the orca",
                    "(C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy",
                    "(D) This person captured a rare sighting of an orca on the beach, so the person was happy"
                ],
                "answer": "(C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy",
                "time_reference": "12:59-13:29",
                "question_type": [
                    "reasoning",
                    "event understanding",
                    "entity recognition"
                ],
                "question": "Why did the person jump up at the end of the clip that has a white reticle at the center?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy",
            "arguments": [
                "Why did the person jump up at the end of the clip that has a white reticle at the center?\nA. (A) This person witnessed people hunting orca on the beach, so the person was angry\nB. (B) This person saw an orca hunting elephant seals on the beach, so he attempted to drive away the orca\nC. (C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy\nD. (D) This person captured a rare sighting of an orca on the beach, so the person was happy\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046e90>"
                    ]
                },
                141,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6ae56c51ecd1dd6abed2d48e0635dd7d16512e9d65cc4ec30824ef4707779226",
            "target_hash": "9938ef843c5f92aa778d3d2d69c3148caf875a009942d436d409196962613d5e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 142,
            "doc": {
                "candidates": [
                    "(A) Looking at the camera",
                    "(B) Following its own parents",
                    "(C) Preening its feathers",
                    "(D) Following the cameraperson"
                ],
                "answer": "(D) Following the cameraperson",
                "time_reference": "18:55-19:15",
                "question_type": [
                    "temporal grounding",
                    "entity recognition"
                ],
                "question": "What are the baby penguins doing from 18:55-19:15?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(D) Following the cameraperson",
            "arguments": [
                "What are the baby penguins doing from 18:55-19:15?\nA. (A) Looking at the camera\nB. (B) Following its own parents\nC. (C) Preening its feathers\nD. (D) Following the cameraperson\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047d60>"
                    ]
                },
                142,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c03474c1424ec9a96fdd034273067fe55c57863b05f3fb5930636d65d4d87407",
            "target_hash": "b140df4cb805d6ee78e71311201f15cce2577e10c05c2e1380ea161e5772096b",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 143,
            "doc": {
                "candidates": [
                    "(A) A group of orcas swims playfully in the sea",
                    "(B) A group of king penguins swims playfully near the shore",
                    "(C) Orcas prey on king penguins",
                    "(D) King penguins gathers on the rocks for rest"
                ],
                "answer": "(C) Orcas prey on king penguins",
                "time_reference": "19:34-21:18",
                "question_type": [
                    "event understanding"
                ],
                "question": "What key event takes place between the first appearance of the baby penguins and the elephant seal sunbathing on the shore?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(C) Orcas prey on king penguins",
            "arguments": [
                "What key event takes place between the first appearance of the baby penguins and the elephant seal sunbathing on the shore?\nA. (A) A group of orcas swims playfully in the sea\nB. (B) A group of king penguins swims playfully near the shore\nC. (C) Orcas prey on king penguins\nD. (D) King penguins gathers on the rocks for rest\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922cc70>"
                    ]
                },
                143,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ac8fac780cbc169f63f1e14b95e34c953eab52fa03e2bee0a73452fd8f3484a2",
            "target_hash": "295312a29ea08832a32356c6ac47c6219b47c70782db2378e3deb84998206f73",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 144,
            "doc": {
                "candidates": [
                    "(A) It is knocked over by a wave",
                    "(B) It is getting ready to enter the sea",
                    "(C) It is preparing to catch fish",
                    "(D) It just finished swimming and comes ashore"
                ],
                "answer": "(A) It is knocked over by a wave",
                "time_reference": "21:56-22:19",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "question": "Why is the penguin lying down at 22:14?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(A) It is knocked over by a wave",
            "arguments": [
                "Why is the penguin lying down at 22:14?\nA. (A) It is knocked over by a wave\nB. (B) It is getting ready to enter the sea\nC. (C) It is preparing to catch fish\nD. (D) It just finished swimming and comes ashore\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd23e0>"
                    ]
                },
                144,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "dffb49465811d81d1d229e2b61c550f249440ac543c96acc4e3b4a215f35133c",
            "target_hash": "138375eb0b58419a5a3ba78d45a9e66b86b8103d89d60b0540a396ade6a18d35",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 145,
            "doc": {
                "candidates": [
                    "(A) Sketching landscapes",
                    "(B) Writing a travel diary",
                    "(C) Calculating the weather on the island",
                    "(D) Recording the behavior of wild animals"
                ],
                "answer": "(D) Recording the behavior of wild animals",
                "time_reference": "35:12-35:12",
                "question_type": [
                    "reasoning",
                    "entity recognition"
                ],
                "question": "What is the man in the scene most likely doing while on the left side, there are a blue sweatshirt, a black T-shirt, a mint towel, and two black and red socks hanging on a rope to dry?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(D) Recording the behavior of wild animals",
            "arguments": [
                "What is the man in the scene most likely doing while on the left side, there are a blue sweatshirt, a black T-shirt, a mint towel, and two black and red socks hanging on a rope to dry?\nA. (A) Sketching landscapes\nB. (B) Writing a travel diary\nC. (C) Calculating the weather on the island\nD. (D) Recording the behavior of wild animals\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3400>"
                    ]
                },
                145,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bd750232735f0f42ad6e12d06ec90be9ab1dfc0e3475d5a8e7cf5ef32e875d21",
            "target_hash": "0a148343e4cc2f42345823b056580109188d68dd64ae5598b7400af1b2b57ed9",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 146,
            "doc": {
                "candidates": [
                    "(A) Though I'm",
                    "(B) I thought",
                    "(C) I feel that",
                    "(D) I'm not sure"
                ],
                "answer": "(D) I'm not sure",
                "time_reference": "49:35-49:35",
                "question_type": [
                    "key information retrieval",
                    "entity recognition"
                ],
                "question": "What is written at the beginning of the last line on the squared notebook?",
                "type": "documentary",
                "video": "uW9mcG0rdLY"
            },
            "target": "(D) I'm not sure",
            "arguments": [
                "What is written at the beginning of the last line on the squared notebook?\nA. (A) Though I'm\nB. (B) I thought\nC. (C) I feel that\nD. (D) I'm not sure\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044550>"
                    ]
                },
                146,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8122fffdff5a9b48e0117441ef2086d98cba4d3f8c993cfb2e3740fd07e0b105",
            "target_hash": "613ad347cb86bf1df73d8df75b71deb278a6f4103deec937f9096f7f75ff0cd9",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 147,
            "doc": {
                "candidates": [
                    "(A) Grey",
                    "(B) White",
                    "(C) Purple",
                    "(D) Beige"
                ],
                "answer": "(B) White",
                "time_reference": "00:00-01:30",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is the color of the sleeveless top one of the four people at the beginning of the video wear?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(B) White",
            "arguments": [
                "What is the color of the sleeveless top one of the four people at the beginning of the video wear?\nA. (A) Grey\nB. (B) White\nC. (C) Purple\nD. (D) Beige\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045270>"
                    ]
                },
                147,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3b57a2bcee822c933e0e694de203f955abdfba0a6fda77cf6c1039fe993e47e9",
            "target_hash": "c74ceb844b82bd28b9fb46026cac5532ac0117b653aaa63d7c1671daa6630c7c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 148,
            "doc": {
                "candidates": [
                    "(A) Because rising sea levels caused the water to inundate the road",
                    "(B) Due to a burst pipe",
                    "(C) Because of melting ice",
                    "(D) Because of heavy rainfall"
                ],
                "answer": "(A) Because rising sea levels caused the water to inundate the road",
                "time_reference": "02:46-02:46",
                "question_type": [
                    "reasoning"
                ],
                "question": "Why is there so much water on the road in the video?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(A) Because rising sea levels caused the water to inundate the road",
            "arguments": [
                "Why is there so much water on the road in the video?\nA. (A) Because rising sea levels caused the water to inundate the road\nB. (B) Due to a burst pipe\nC. (C) Because of melting ice\nD. (D) Because of heavy rainfall\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046050>"
                    ]
                },
                148,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "dcbf20b540c20b8af7910d3e091cb3d9bc187b2b79a5a20d96eea1002a9bcde6",
            "target_hash": "dcf5e7e0e845715f7b65e62b9cf289d1b2555b9a34046b92f20e6ab63d8864e1",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 149,
            "doc": {
                "candidates": [
                    "(A) To inform the audience that some areas here were not submerged by water in the past",
                    "(B) To demonstrate their photography hobby",
                    "(C) To share memories of their childhood",
                    "(D) To showcase their cooking skills"
                ],
                "answer": "(A) To inform the audience that some areas here were not submerged by water in the past",
                "time_reference": "05:13-05:59",
                "question_type": [
                    "reasoning",
                    "entity recognition"
                ],
                "question": "What is the intention behind the residents showing the photos in the video?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(A) To inform the audience that some areas here were not submerged by water in the past",
            "arguments": [
                "What is the intention behind the residents showing the photos in the video?\nA. (A) To inform the audience that some areas here were not submerged by water in the past\nB. (B) To demonstrate their photography hobby\nC. (C) To share memories of their childhood\nD. (D) To showcase their cooking skills\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046fb0>"
                    ]
                },
                149,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "adabead6b3dfc6501d5282ec1e7bdd167443e28245273467fa4cdac366ff6271",
            "target_hash": "96efdd2e45e06f0c941571d5d2732b0f3801f851a6572e0e41a567cb9c9ae014",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 150,
            "doc": {
                "candidates": [
                    "(A) Coconut",
                    "(B) Cherry",
                    "(C) Apple",
                    "(D) Blueberry"
                ],
                "answer": "(A) Coconut",
                "time_reference": "05:51-05:53",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Which fruit is shown in the video?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(A) Coconut",
            "arguments": [
                "Which fruit is shown in the video?\nA. (A) Coconut\nB. (B) Cherry\nC. (C) Apple\nD. (D) Blueberry\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047e80>"
                    ]
                },
                150,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9ef107c7a69fed4fb0b011c872bab613dd34e315d7f83dab176ac9b116b6f61d",
            "target_hash": "ac9379dec4b12fd31db8780db303eba72b948f190b8924aeecc274c96b231e08",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 151,
            "doc": {
                "candidates": [
                    "(A) Mother and daughter",
                    "(B) School principle and student",
                    "(C) Grandmother and granddaughter",
                    "(D) Teacher and student"
                ],
                "answer": "(C) Grandmother and granddaughter",
                "time_reference": "06:42-06:42",
                "question_type": [
                    "entity recognition",
                    "reasoning"
                ],
                "question": "What is the relation between the girl wearing a school uniform and the woman wearing a floral top and a purple dress?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(C) Grandmother and granddaughter",
            "arguments": [
                "What is the relation between the girl wearing a school uniform and the woman wearing a floral top and a purple dress?\nA. (A) Mother and daughter\nB. (B) School principle and student\nC. (C) Grandmother and granddaughter\nD. (D) Teacher and student\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922cd90>"
                    ]
                },
                151,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "24d6eb03c7b458f277acf4e79ba2c011a4b1ee1ea50faaa29182cca476bbda59",
            "target_hash": "5a740c7049e05a60c5df34c7a920c88341f7e35aa1ddaafa60eb6d57a342722e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 152,
            "doc": {
                "candidates": [
                    "(A) Honeydew",
                    "(B) Pumpkin",
                    "(C) Cantaloupe",
                    "(D) Watermelon"
                ],
                "answer": "(C) Cantaloupe",
                "time_reference": "22:04-22:13",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What type of melon is cut?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(C) Cantaloupe",
            "arguments": [
                "What type of melon is cut?\nA. (A) Honeydew\nB. (B) Pumpkin\nC. (C) Cantaloupe\nD. (D) Watermelon\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2500>"
                    ]
                },
                152,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9b789dcbfeaec9d943d3ccc48a9f4c6829e2f732520c3e0196394e62f4346425",
            "target_hash": "8f28ce14b00027ab51ded961c61b447c5421344eb2c8c9430f5eb32e9d3f8bac",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 153,
            "doc": {
                "candidates": [
                    "(A) They are females",
                    "(B) They sing while cooking",
                    "(C) They wear hats",
                    "(D) They use the same type of utensils"
                ],
                "answer": "(A) They are females",
                "time_reference": "22:00-23:40",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "question": "What do the people preparing food in the video have in common?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(A) They are females",
            "arguments": [
                "What do the people preparing food in the video have in common?\nA. (A) They are females\nB. (B) They sing while cooking\nC. (C) They wear hats\nD. (D) They use the same type of utensils\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3520>"
                    ]
                },
                153,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "592ffc8007a90378c664745324fb9dc393ded2f1f33aaf89bff1d3f2fdcb3903",
            "target_hash": "df4e80e45ec9357363f5c0fb992982860c3bfd2920408e8a788125de073db6da",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 154,
            "doc": {
                "candidates": [
                    "(A) No",
                    "(B) Yes",
                    "(C) Currently in contact with the outside world",
                    "(D) Not sure"
                ],
                "answer": "(B) Yes",
                "time_reference": "27:00-27:09",
                "question_type": [
                    "reasoning",
                    "event understanding"
                ],
                "question": "Did the islanders accept help from the outside?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(B) Yes",
            "arguments": [
                "Did the islanders accept help from the outside?\nA. (A) No\nB. (B) Yes\nC. (C) Currently in contact with the outside world\nD. (D) Not sure\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044670>"
                    ]
                },
                154,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2ed75602f1477303934c7105578aee51e07b116e1eb5e1aa83dee3ceccfa9314",
            "target_hash": "ca4dde3dc6a6db93e265e665454c4b09148d8a5106829faa234e1702d4369845",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 155,
            "doc": {
                "candidates": [
                    "(A) Running",
                    "(B) Walking",
                    "(C) Riding a bicycle",
                    "(D) Swimming"
                ],
                "answer": "(C) Riding a bicycle",
                "time_reference": "27:13-27:13",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "question": "What is the person doing while passing by the sign that has \"Climate Change Adaptation Support Project Pilot Site\" written on it?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(C) Riding a bicycle",
            "arguments": [
                "What is the person doing while passing by the sign that has \"Climate Change Adaptation Support Project Pilot Site\" written on it?\nA. (A) Running\nB. (B) Walking\nC. (C) Riding a bicycle\nD. (D) Swimming\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045390>"
                    ]
                },
                155,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4af61b91f7944b2e4ab71f9c1d6aea529b8a0b922edbbe7c5f18aa6541179c02",
            "target_hash": "8769d39764bc0358e6026c86cc8e7109d7c81d915b68d864e584b4da9c4f0b3c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 156,
            "doc": {
                "candidates": [
                    "(A) 9964",
                    "(B) 9946",
                    "(C) 9679",
                    "(D) 9619"
                ],
                "answer": "(D) 9619",
                "time_reference": "28:13-28:13",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is the number of the student trainer that is further away from the camera compared to the 9772 student trainer?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(D) 9619",
            "arguments": [
                "What is the number of the student trainer that is further away from the camera compared to the 9772 student trainer?\nA. (A) 9964\nB. (B) 9946\nC. (C) 9679\nD. (D) 9619\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046170>"
                    ]
                },
                156,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c4e81f78b74045d8da42580c4c59340e25b0903adbb91f0edc4de83d0d3ee51b",
            "target_hash": "438dde9bfce74bff2f83db2008c7dbcbe94ed271c40b0075a7558e04b4cf365b",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 157,
            "doc": {
                "candidates": [
                    "(A) At the beach",
                    "(B) In the kitchen",
                    "(C) In the dormitory",
                    "(D) In the forest"
                ],
                "answer": "(C) In the dormitory",
                "time_reference": "31:00-31:21",
                "question_type": [
                    "event understanding"
                ],
                "question": "Where is the person with the number 9615 on their chest being interviewed in the video?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(C) In the dormitory",
            "arguments": [
                "Where is the person with the number 9615 on their chest being interviewed in the video?\nA. (A) At the beach\nB. (B) In the kitchen\nC. (C) In the dormitory\nD. (D) In the forest\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0470d0>"
                    ]
                },
                157,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "07b143656fa93123c7ff1bde66308a53d83dcd6eef8cb3d0ab2d38e92760d507",
            "target_hash": "36314713baa75a62c5f2f270a7da4aaf1b11b6995961330c43fbc81bdd737944",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 158,
            "doc": {
                "candidates": [
                    "(A) Yellow",
                    "(B) White",
                    "(C) Black",
                    "(D) Blue"
                ],
                "answer": "(A) Yellow",
                "time_reference": "33:00-33:00",
                "question_type": [
                    "entity recognition",
                    "reasoning"
                ],
                "question": "What color tops are most of the elementary school students wearing in the video?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(A) Yellow",
            "arguments": [
                "What color tops are most of the elementary school students wearing in the video?\nA. (A) Yellow\nB. (B) White\nC. (C) Black\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047fa0>"
                    ]
                },
                158,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ad8d81e0b4be1493e0b3fccffe737749d80f1e3d6c07b9cbc1f9a63cdc649223",
            "target_hash": "17db5ef3256df7c9256c75ba0516a35208ef27948344ccc3af6325bf80ace62e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 159,
            "doc": {
                "candidates": [
                    "(A) They sit on tables",
                    "(B) They sit on chairs",
                    "(C) They sit on the floor",
                    "(D) They stand"
                ],
                "answer": "(C) They sit on the floor",
                "time_reference": "33:33-33:34",
                "question_type": [
                    "event understanding"
                ],
                "question": "Where do the elementary school students sit when the teacher is teaching?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(C) They sit on the floor",
            "arguments": [
                "Where do the elementary school students sit when the teacher is teaching?\nA. (A) They sit on tables\nB. (B) They sit on chairs\nC. (C) They sit on the floor\nD. (D) They stand\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922ceb0>"
                    ]
                },
                159,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0b304a1a2f93329b100bdee66b0b71b238c3da00c4aa115651fcf65a443d8f71",
            "target_hash": "47c8cde8c4aa6750accb161bf30457f54dc501d6157991905d12d89aee4e4b77",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 160,
            "doc": {
                "candidates": [
                    "(A) They are hiking",
                    "(B) They are fishing",
                    "(C) They are hunting",
                    "(D) They are cooking"
                ],
                "answer": "(B) They are fishing",
                "time_reference": "38:05-38:58",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What are the people holding a torch in their hand doing at night?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(B) They are fishing",
            "arguments": [
                "What are the people holding a torch in their hand doing at night?\nA. (A) They are hiking\nB. (B) They are fishing\nC. (C) They are hunting\nD. (D) They are cooking\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2620>"
                    ]
                },
                160,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ae30367298002272249b9d6b9b08ee2de3e6fb229b262a9a7e75da3ff73582ed",
            "target_hash": "8b26b6bb471d62297c2065601b49def0dbc19d8648cccca784a8aecfa821692f",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 161,
            "doc": {
                "candidates": [
                    "(A) Blue",
                    "(B) Red",
                    "(C) Black",
                    "(D) Yellow"
                ],
                "answer": "(D) Yellow",
                "time_reference": "39:21-39:21",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is the oxygen tank that appears the last in the video?",
                "type": "documentary",
                "video": "TZ0j6kr4ZJ0"
            },
            "target": "(D) Yellow",
            "arguments": [
                "What color is the oxygen tank that appears the last in the video?\nA. (A) Blue\nB. (B) Red\nC. (C) Black\nD. (D) Yellow\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3640>"
                    ]
                },
                161,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "44e1248488027d12fe2d4fd8a7ffcc81f1d5426927dca7300382caf074b9331a",
            "target_hash": "0b742116fd1f4311d53c40fdbfbf2f5ef15ee874a453ec11b4e98eb3b1a02775",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 162,
            "doc": {
                "candidates": [
                    "(A) Stephen Winklemann",
                    "(B) Chris Piochon",
                    "(C) Christophe Piochon",
                    "(D) Stephan Winklemann"
                ],
                "answer": "(D) Stephan Winklemann",
                "time_reference": "03:19-03:25",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "Who is the president of Bugatti?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) Stephan Winklemann",
            "arguments": [
                "Who is the president of Bugatti?\nA. (A) Stephen Winklemann\nB. (B) Chris Piochon\nC. (C) Christophe Piochon\nD. (D) Stephan Winklemann\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044790>"
                    ]
                },
                162,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "69dd0598f4d41fd5f0f997b74617f146501581386138d340c6802bcac81cf2c1",
            "target_hash": "2987ed5f1705bdd628cd6cae017f63df3eca7c8debdef4f85d3be495b54dc21c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 163,
            "doc": {
                "candidates": [
                    "(A) Blue",
                    "(B) White",
                    "(C) Grey",
                    "(D) Black"
                ],
                "answer": "(D) Black",
                "time_reference": "04:18-04:32",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color are the staff uniforms of Bugatti workers?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) Black",
            "arguments": [
                "What color are the staff uniforms of Bugatti workers?\nA. (A) Blue\nB. (B) White\nC. (C) Grey\nD. (D) Black\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0455d0>"
                    ]
                },
                163,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d85e40b02a8788def310fc67dd8b62ef588ab4f185823ec15b7f3ebfb53d4e51",
            "target_hash": "1def1ffd02b2328d50ae6a5baad3bbc3bf9cde1e18e001fe55fdc513f415383a",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 164,
            "doc": {
                "candidates": [
                    "(A) They stand on the chassis",
                    "(B) They put car parts on the chassis",
                    "(C) They put the tools on the chassis",
                    "(D) They put the car shell on the chassis"
                ],
                "answer": "(D) They put the car shell on the chassis",
                "time_reference": "05:19-05:33",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "question": "What do workers do after they put up the chassis in 05:19?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) They put the car shell on the chassis",
            "arguments": [
                "What do workers do after they put up the chassis in 05:19?\nA. (A) They stand on the chassis\nB. (B) They put car parts on the chassis\nC. (C) They put the tools on the chassis\nD. (D) They put the car shell on the chassis\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046290>"
                    ]
                },
                164,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "552bf9e6cb29bf4de59df49f41ec10fec348fcc065e42ca1ad0c36af801b1da3",
            "target_hash": "a48ae33a6797560eaa68bbc8a42548b64739940ce83ecda937f424a4250abdef",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 165,
            "doc": {
                "candidates": [
                    "(A) They are testing different engines",
                    "(B) They are putting the engine into the car",
                    "(C) They are installing the turbochargers",
                    "(D) They are assembling the engine"
                ],
                "answer": "(D) They are assembling the engine",
                "time_reference": "07:30-14:12",
                "question_type": [
                    "temporal grounding",
                    "event understanding"
                ],
                "question": "What are the workers doing from 07:30-14:12?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) They are assembling the engine",
            "arguments": [
                "What are the workers doing from 07:30-14:12?\nA. (A) They are testing different engines\nB. (B) They are putting the engine into the car\nC. (C) They are installing the turbochargers\nD. (D) They are assembling the engine\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0471f0>"
                    ]
                },
                165,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cf20a9df8c82ed04c04ce5140f35276fecdce9a10c85299120648af6c9db0e52",
            "target_hash": "950f19e123128a541aa6a24c36866570b9eddf3650ed43d48d7e2e818266eebc",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 166,
            "doc": {
                "candidates": [
                    "(A) He is testing the sound of the engine",
                    "(B) He is testing the exhaust gas from the engine",
                    "(C) He is testing the temperature of the engine",
                    "(D) He is testing the power of the engine"
                ],
                "answer": "(A) He is testing the sound of the engine",
                "time_reference": "18:59-20:30",
                "question_type": [
                    "event understanding"
                ],
                "question": "What does Dr. Conrad test?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(A) He is testing the sound of the engine",
            "arguments": [
                "What does Dr. Conrad test?\nA. (A) He is testing the sound of the engine\nB. (B) He is testing the exhaust gas from the engine\nC. (C) He is testing the temperature of the engine\nD. (D) He is testing the power of the engine\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047fd0>"
                    ]
                },
                166,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ef7e90ad0f159dc16f2d7a7a15c5c44f6d86581b510427bd552b9deb099a4d8e",
            "target_hash": "7efdd86052b8da63e9aa1d4fb38c832402469d36130c7559a3accff02bf58b3c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 167,
            "doc": {
                "candidates": [
                    "(A) He puts on his coat",
                    "(B) He checks his car",
                    "(C) He takes off his coat",
                    "(D) He finds his car key"
                ],
                "answer": "(A) He puts on his coat",
                "time_reference": "20:40-20:50",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "question": "What does the test driver do before he enters the car with license plate \"W392DB\"?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(A) He puts on his coat",
            "arguments": [
                "What does the test driver do before he enters the car with license plate \"W392DB\"?\nA. (A) He puts on his coat\nB. (B) He checks his car\nC. (C) He takes off his coat\nD. (D) He finds his car key\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922cfd0>"
                    ]
                },
                167,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5d2b28cb2311b23cedf6dcc41e088422c4c1f36f2abfabd254851598df086d77",
            "target_hash": "23259aced898d1fd9c1178ccef2f9d59c7cb57256f53175b10f77537b46707cf",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 168,
            "doc": {
                "candidates": [
                    "(A) Black with three orange stripes on the top",
                    "(B) Orange with two black stripes on the top",
                    "(C) Orange with three black stripes on the top",
                    "(D) Black with two orange stripes on the top"
                ],
                "answer": "(D) Black with two orange stripes on the top",
                "time_reference": "21:35-21:50",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What colour is the car Andy test?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) Black with two orange stripes on the top",
            "arguments": [
                "What colour is the car Andy test?\nA. (A) Black with three orange stripes on the top\nB. (B) Orange with two black stripes on the top\nC. (C) Orange with three black stripes on the top\nD. (D) Black with two orange stripes on the top\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2740>"
                    ]
                },
                168,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1f8c5c5e3c4aae25b2c8150c4f169212806348e96d5434977814bcac5069d955",
            "target_hash": "77330d9ae729ee0e32cffc25bdd6ca76c3f96bbbb8640112ca999e5463e07c70",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 169,
            "doc": {
                "candidates": [
                    "(A) 304.47MPH",
                    "(B) 307.47MPH",
                    "(C) 307.44 MPH",
                    "(D) 304.77 MPH"
                ],
                "answer": "(D) 304.77 MPH",
                "time_reference": "21:35-23:34",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What is Andy's top speed while testing the sports car?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) 304.77 MPH",
            "arguments": [
                "What is Andy's top speed while testing the sports car?\nA. (A) 304.47MPH\nB. (B) 307.47MPH\nC. (C) 307.44 MPH\nD. (D) 304.77 MPH\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3760>"
                    ]
                },
                169,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0f19ee2ed5254dede5f54b42b42938e321ec3bebd6d1f3c059fa4dc5c37df875",
            "target_hash": "75b34cbb78d4acf3cbb41ecc7cff074a1a53ca9924a3b7185c04a3977275b146",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 170,
            "doc": {
                "candidates": [
                    "(A) Because they really like the car",
                    "(B) Because Andy shows off his skills during the test drive",
                    "(C) Because Andy drives to a very fast speed",
                    "(D) Because there is no accident during the test drive"
                ],
                "answer": "(C) Because Andy drives to a very fast speed",
                "time_reference": "23:39-23:50",
                "question_type": [
                    "reasoning"
                ],
                "question": "Why is everyone celebrating after the test drive is over?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(C) Because Andy drives to a very fast speed",
            "arguments": [
                "Why is everyone celebrating after the test drive is over?\nA. (A) Because they really like the car\nB. (B) Because Andy shows off his skills during the test drive\nC. (C) Because Andy drives to a very fast speed\nD. (D) Because there is no accident during the test drive\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0448b0>"
                    ]
                },
                170,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b8e7ef9270c21a3f0e17ae33542a362cd82907f4461b520f6eed2fd667e7a3bd",
            "target_hash": "83ad8ae37547fcb5db998949ec9f93db76f796f53a0e12fca533867a7bfb27c0",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 171,
            "doc": {
                "candidates": [
                    "(A) Controlled by computer",
                    "(B) Rely on manpower",
                    "(C) Rely on a forklift",
                    "(D) Rely on gravity"
                ],
                "answer": "(B) Rely on manpower",
                "time_reference": "32:28-33:08",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How are the two parts, one for the front seats and one for the back seats, put together and then assembled?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(B) Rely on manpower",
            "arguments": [
                "How are the two parts, one for the front seats and one for the back seats, put together and then assembled?\nA. (A) Controlled by computer\nB. (B) Rely on manpower\nC. (C) Rely on a forklift\nD. (D) Rely on gravity\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac044e80>"
                    ]
                },
                171,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d117c9fef6c335f656724c05d3c8cf89508b571bcd3ade304837ab4f902faa89",
            "target_hash": "3a83495094fd9a84a70b1b16cb9d4c82d142e9b919fc19c38a5b6a5d8d399c0b",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 172,
            "doc": {
                "candidates": [
                    "(A) Left rear wheel",
                    "(B) Left front wheel",
                    "(C) Right rear wheel",
                    "(D) Right front wheel"
                ],
                "answer": "(C) Right rear wheel",
                "time_reference": "34:31-35:12",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Which tire is put on first during assembly?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(C) Right rear wheel",
            "arguments": [
                "Which tire is put on first during assembly?\nA. (A) Left rear wheel\nB. (B) Left front wheel\nC. (C) Right rear wheel\nD. (D) Right front wheel\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0463b0>"
                    ]
                },
                172,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "352625a42bc4c2d779f57c81311062fe5bdaee7553510f58040d88f25f09e6dc",
            "target_hash": "2d22a705e874befebffbcab4f3753152658cb305977552b47e7a4c36b184a434",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 173,
            "doc": {
                "candidates": [
                    "(A) Add various fluids to the car, including engine oil and gasoline",
                    "(B) Install other casings on the car",
                    "(C) Test drive",
                    "(D) Power up the car"
                ],
                "answer": "(A) Add various fluids to the car, including engine oil and gasoline",
                "time_reference": "35:00-36:00",
                "question_type": [
                    "event understanding"
                ],
                "question": "What is the installation process after installing the tires?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(A) Add various fluids to the car, including engine oil and gasoline",
            "arguments": [
                "What is the installation process after installing the tires?\nA. (A) Add various fluids to the car, including engine oil and gasoline\nB. (B) Install other casings on the car\nC. (C) Test drive\nD. (D) Power up the car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047310>"
                    ]
                },
                173,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5af9ccfb1d1fd958d5b3d65c458789d56316fd7938cf4b8da3f0f6e327e6241b",
            "target_hash": "5328492e52f7cf03635b94978d93bd86791d3b84a334c484fc433ccd569c7467",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 174,
            "doc": {
                "candidates": [
                    "(A) Maxium Born",
                    "(B) Maxime Bohn",
                    "(C) Maxime Born",
                    "(D) Maxium Bohn"
                ],
                "answer": "(B) Maxime Bohn",
                "time_reference": "38:14-38:02",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "Who is the assembly editor of Bugatti?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(B) Maxime Bohn",
            "arguments": [
                "Who is the assembly editor of Bugatti?\nA. (A) Maxium Born\nB. (B) Maxime Bohn\nC. (C) Maxime Born\nD. (D) Maxium Bohn\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046c80>"
                    ]
                },
                174,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d8620c990a84f60cdcf7183f36509d32823038a46e8743b6fb83753ab1360796",
            "target_hash": "b2370d36e88d43d4419e9b3d677b908466c37333daedaf2315872cfe55266fa5",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 175,
            "doc": {
                "candidates": [
                    "(A) Black",
                    "(B) Silver",
                    "(C) Gold",
                    "(D) White"
                ],
                "answer": "(D) White",
                "time_reference": "38:25-39:27",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What color is the car they assemble?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(D) White",
            "arguments": [
                "What color is the car they assemble?\nA. (A) Black\nB. (B) Silver\nC. (C) Gold\nD. (D) White\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922d0f0>"
                    ]
                },
                175,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6112df299a0a0cc2ba6b3ed635dfd5855f4570e83dc1691c0b005389b8ad8454",
            "target_hash": "ca4aeab00311a07ae35d4347bff25bdba0921bc404e350d70d22d507d40243dd",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 176,
            "doc": {
                "candidates": [
                    "(A) The blue dotted line represents cold wind and the red dotted line represents hot wind",
                    "(B) The blue dotted line outlines the shape of the front half of the car, and the red dotted line outlines the shape of the rear half of the car",
                    "(C) The blue dotted line represents where reach the wind and the red dotted line represents where will not hit by wind",
                    "(D) The blue dotted line represents strong wind and the red dotted line represents weak wind"
                ],
                "answer": "(A) The blue dotted line represents cold wind and the red dotted line represents hot wind",
                "time_reference": "41:58-42:10",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "question": "What do the blue and red dotted lines in the animation represent at 42:04?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(A) The blue dotted line represents cold wind and the red dotted line represents hot wind",
            "arguments": [
                "What do the blue and red dotted lines in the animation represent at 42:04?\nA. (A) The blue dotted line represents cold wind and the red dotted line represents hot wind\nB. (B) The blue dotted line outlines the shape of the front half of the car, and the red dotted line outlines the shape of the rear half of the car\nC. (C) The blue dotted line represents where reach the wind and the red dotted line represents where will not hit by wind\nD. (D) The blue dotted line represents strong wind and the red dotted line represents weak wind\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2860>"
                    ]
                },
                176,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ff9e6808d218e6862ddadc3a0a0e93ac4f2fbfa5f23052ce9e09fc5f9f01a409",
            "target_hash": "1cce37222f3603eeeddb3ec93c5da943e4ad05d3e007e54ad774d50b6674f575",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 177,
            "doc": {
                "candidates": [
                    "(A) Snowy",
                    "(B) Sunny",
                    "(C) Cloudy",
                    "(D) Rainny"
                ],
                "answer": "(C) Cloudy",
                "time_reference": "47:02-48:15",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the weather when the last test drive?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(C) Cloudy",
            "arguments": [
                "What is the weather when the last test drive?\nA. (A) Snowy\nB. (B) Sunny\nC. (C) Cloudy\nD. (D) Rainny\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3880>"
                    ]
                },
                177,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "556bb0c0b34d2009bddf95dc691be97942a88f2baf700510794f544135d2aeec",
            "target_hash": "396cefd975128fde7cba4b28cc7d000982f723effec5849e69fc9248de00f1e0",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 178,
            "doc": {
                "candidates": [
                    "(A) 13",
                    "(B) 10",
                    "(C) 7",
                    "(D) 16"
                ],
                "answer": "(B) 10",
                "time_reference": "00:00-49:04",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many people are interviewed in this video?",
                "type": "documentary",
                "video": "ihfjEFGdZdc"
            },
            "target": "(B) 10",
            "arguments": [
                "How many people are interviewed in this video?\nA. (A) 13\nB. (B) 10\nC. (C) 7\nD. (D) 16\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0449d0>"
                    ]
                },
                178,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0f32ed60ea9370dd68a7af6e45399921dfb67daecff2d44be65c722f6a0cce3a",
            "target_hash": "f3db718437d9244cf730435bce31c8879a5d14a75c8156262698c2dc0edd786c",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 179,
            "doc": {
                "candidates": [
                    "(A) Star style earrings",
                    "(B) Diamond earrings",
                    "(C) Pearl style earrings",
                    "(D) Rose style earrings"
                ],
                "answer": "(C) Pearl style earrings",
                "time_reference": "06:01-06:13",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What type of earrings does the woman who sits at the steering wheel wear?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(C) Pearl style earrings",
            "arguments": [
                "What type of earrings does the woman who sits at the steering wheel wear?\nA. (A) Star style earrings\nB. (B) Diamond earrings\nC. (C) Pearl style earrings\nD. (D) Rose style earrings\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0446d0>"
                    ]
                },
                179,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "684ff9b375021604fc3e66a085bff7cddb8984dac190c363805be3d35647d7e5",
            "target_hash": "bef77d6945f11602f05f6ab6da9c0191a82f5704ed1def3867ea78eb15ec08d9",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 180,
            "doc": {
                "candidates": [
                    "(A) She quickly picked up a gun and shot at the sea",
                    "(B) She put on sunglasses and shot at the sea with a gun",
                    "(C) She came back after touring the ship's safety problems",
                    "(D) She wear a hard hat and shoot into the sea with a gun"
                ],
                "answer": "(D) She wear a hard hat and shoot into the sea with a gun",
                "time_reference": "06:17-06:27",
                "question_type": [
                    "event understanding"
                ],
                "question": "What did the woman with the pearl earrings do when she got down the ladder?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(D) She wear a hard hat and shoot into the sea with a gun",
            "arguments": [
                "What did the woman with the pearl earrings do when she got down the ladder?\nA. (A) She quickly picked up a gun and shot at the sea\nB. (B) She put on sunglasses and shot at the sea with a gun\nC. (C) She came back after touring the ship's safety problems\nD. (D) She wear a hard hat and shoot into the sea with a gun\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0464d0>"
                    ]
                },
                180,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f04420b9d8f4716cb6ff40df2f390537aeeb876e324537da04972331a2cc5e50",
            "target_hash": "a732ab9534bb3cc44686b05cc97bad0f09cf4ac1148363e9e3b4c0cd01aadcb2",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 181,
            "doc": {
                "candidates": [
                    "(A) Interphone",
                    "(B) Telephone",
                    "(C) Remote control",
                    "(D) Telescope"
                ],
                "answer": "(D) Telescope",
                "time_reference": "29:33-29:48",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What's the man in black short sleeves holding?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(D) Telescope",
            "arguments": [
                "What's the man in black short sleeves holding?\nA. (A) Interphone\nB. (B) Telephone\nC. (C) Remote control\nD. (D) Telescope\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047430>"
                    ]
                },
                181,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "632c76babc9555c03c558f64d3ef0862c72fe344f14bf79b918d3ad683526bc0",
            "target_hash": "6ca9439e646c2d7a64707c169a0de3346166097e38b95928182a0c39c11a24d7",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 182,
            "doc": {
                "candidates": [
                    "(A) 3",
                    "(B) 2",
                    "(C) 1",
                    "(D) 4"
                ],
                "answer": "(A) 3",
                "time_reference": "31:15-31:16",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many people are there in the descending elevator?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(A) 3",
            "arguments": [
                "How many people are there in the descending elevator?\nA. (A) 3\nB. (B) 2\nC. (C) 1\nD. (D) 4\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046ec0>"
                    ]
                },
                182,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "47f00eb9bcf8b31974c9d40601a0fb352491fa07343ac1e75a76a16b1204aa3b",
            "target_hash": "9dd873a4c7f0a878dd73cc9b87bf5615b5daa1a2e6d25c9364d84c064d1b5854",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 183,
            "doc": {
                "candidates": [
                    "(A) SERGEY SARANCHA",
                    "(B) PAUL AGATE",
                    "(C) ALI ARIF",
                    "(D) ROGER MIDDLETON"
                ],
                "answer": "(D) ROGER MIDDLETON",
                "time_reference": "31:20-31:44",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "question": "Who was the man who spoke between LORETTA NAPOLEONI and ATEPHEN ASKINS?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(D) ROGER MIDDLETON",
            "arguments": [
                "Who was the man who spoke between LORETTA NAPOLEONI and ATEPHEN ASKINS?\nA. (A) SERGEY SARANCHA\nB. (B) PAUL AGATE\nC. (C) ALI ARIF\nD. (D) ROGER MIDDLETON\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922d210>"
                    ]
                },
                183,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6371916af37e8afd48ab528bae158acf6a54b1d5ad2121c5c8aab6f8a31ebdd9",
            "target_hash": "bf7bd2d11a5154d99d2b94d5caf2f4ab1b1f0cbf31aa7969bee92afd19232bf6",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 184,
            "doc": {
                "candidates": [
                    "(A) 33",
                    "(B) 03",
                    "(C) 30",
                    "(D) 32"
                ],
                "answer": "(A) 33",
                "time_reference": "32:01-32:05",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What was written on the right glass door when Paul Agate entered the front door?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(A) 33",
            "arguments": [
                "What was written on the right glass door when Paul Agate entered the front door?\nA. (A) 33\nB. (B) 03\nC. (C) 30\nD. (D) 32\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2980>"
                    ]
                },
                184,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8eea90bb54a8849e49e13590f0bd62f6b9d8d49af70b7c5a5c56ec050a137b76",
            "target_hash": "3d56b3fa17bb28049479b4c1910eb05e324cea0d186af102f6192d91f204a310",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 185,
            "doc": {
                "candidates": [
                    "(A) 10",
                    "(B) 12",
                    "(C) 11",
                    "(D) 13"
                ],
                "answer": "(C) 11",
                "time_reference": "32:51-33:00",
                "question_type": [
                    "entity recognition"
                ],
                "question": "How many people dragged the blue and white wooden boat ashore at the same time?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(C) 11",
            "arguments": [
                "How many people dragged the blue and white wooden boat ashore at the same time?\nA. (A) 10\nB. (B) 12\nC. (C) 11\nD. (D) 13\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd39a0>"
                    ]
                },
                185,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "be8c87138b5951bd641f64b3667ffb652520643df745d8a699931bd171c16e72",
            "target_hash": "653a00f9bbed6bafe4ef76a29a9ef720dd8998d86d4b1b1de989e516cc3fe8e4",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 186,
            "doc": {
                "candidates": [
                    "(A) 18",
                    "(B) 25",
                    "(C) 5",
                    "(D) 10"
                ],
                "answer": "(B) 25",
                "time_reference": "39:16-39:25",
                "question_type": [
                    "key information retrieval"
                ],
                "question": "What number is highlighted with rectangle in the chart written with pencil in numerical increasing order for each name?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(B) 25",
            "arguments": [
                "What number is highlighted with rectangle in the chart written with pencil in numerical increasing order for each name?\nA. (A) 18\nB. (B) 25\nC. (C) 5\nD. (D) 10\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3d60>"
                    ]
                },
                186,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "28ecb598c64b3e74ed894d907553d24a4a0c51f7eddaf2b81b994889d8d3bf6f",
            "target_hash": "441a28e650a9ea5bb0939a76b079b1ad5b9d38b0b8da13c5fcefa87bf7488e61",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 187,
            "doc": {
                "candidates": [
                    "(A) There are eight floors",
                    "(B) There are five floors",
                    "(C) There are six floors",
                    "(D) There are four floors"
                ],
                "answer": "(B) There are five floors",
                "time_reference": "41:28-41:35",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Before Svetlana McGlaser spoke, how many floors did the building in the video have?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(B) There are five floors",
            "arguments": [
                "Before Svetlana McGlaser spoke, how many floors did the building in the video have?\nA. (A) There are eight floors\nB. (B) There are five floors\nC. (C) There are six floors\nD. (D) There are four floors\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac045690>"
                    ]
                },
                187,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3c02a881975b8bf1b1fa473cdc50b98b0b5f1182b4b810ede5e3fd8de4d30271",
            "target_hash": "9a4ce3dadaf73e80fe3d59bf4e6c1b2f6e2f9eecfdf9f57d228a638d21780d09",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 188,
            "doc": {
                "candidates": [
                    "(A) White",
                    "(B) Yellow",
                    "(C) Red",
                    "(D) Green"
                ],
                "answer": "(D) Green",
                "time_reference": "45:37-45:42",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the background color of \"EXIT\" on the door in the picture when the bearded man speaks?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(D) Green",
            "arguments": [
                "What is the background color of \"EXIT\" on the door in the picture when the bearded man speaks?\nA. (A) White\nB. (B) Yellow\nC. (C) Red\nD. (D) Green\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0465f0>"
                    ]
                },
                188,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "84f18e0fb3b062f97e8800b6cc66e97e8547e96473d3d3f0c87ca91fee871c46",
            "target_hash": "1467faa1684c871eccb61d0fe03dff196c5c3cb3e35d92428a306428f10f86fc",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 189,
            "doc": {
                "candidates": [
                    "(A) A man named Hotten wears goggles",
                    "(B) A man named Hotten fired a shot",
                    "(C) A man named Helen fired a shot",
                    "(D) A man named Helen put on goggles"
                ],
                "answer": "(B) A man named Hotten fired a shot",
                "time_reference": "47:26-47:40",
                "question_type": [
                    "event understanding"
                ],
                "question": "What happens in the next frame when the red helicopter flies by?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(B) A man named Hotten fired a shot",
            "arguments": [
                "What happens in the next frame when the red helicopter flies by?\nA. (A) A man named Hotten wears goggles\nB. (B) A man named Hotten fired a shot\nC. (C) A man named Helen fired a shot\nD. (D) A man named Helen put on goggles\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac047550>"
                    ]
                },
                189,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D",
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e37a4b8462407d427091a225dc08da33242e3b39b65950cb1b6830ba0b9707b3",
            "target_hash": "163781e77e956efcd5b4db36e1518d79cc9a57b7261a2d846513a2032043232c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 190,
            "doc": {
                "candidates": [
                    "(A) 3",
                    "(B) 4",
                    "(C) 6",
                    "(D) 1"
                ],
                "answer": "(D) 1",
                "time_reference": "48:12-48:20",
                "question_type": [
                    "entity recognition"
                ],
                "question": "Before GEIR-OVE spoke, how many of the runners had bottled water?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(D) 1",
            "arguments": [
                "Before GEIR-OVE spoke, how many of the runners had bottled water?\nA. (A) 3\nB. (B) 4\nC. (C) 6\nD. (D) 1\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922c460>"
                    ]
                },
                190,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A",
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e1c90b5b4cb8540502d114735316c4402762092d3a501e71192244eb1dca7a57",
            "target_hash": "130e553bd423b938999fb170663c3db89fe536737e0598ae8b8ff9079e800be9",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 191,
            "doc": {
                "candidates": [
                    "(A) He clinked glasses with everyone",
                    "(B) He helped the others with their dishes",
                    "(C) He took off his braces and gargled",
                    "(D) He distributed chopsticks to everyone"
                ],
                "answer": "(A) He clinked glasses with everyone",
                "time_reference": "51:47-52:18",
                "question_type": [
                    "event understanding"
                ],
                "question": "What did Sergey Sarancha do before eating?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(A) He clinked glasses with everyone",
            "arguments": [
                "What did Sergey Sarancha do before eating?\nA. (A) He clinked glasses with everyone\nB. (B) He helped the others with their dishes\nC. (C) He took off his braces and gargled\nD. (D) He distributed chopsticks to everyone\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f360922d330>"
                    ]
                },
                191,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C",
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d46dd8f9b871d812f711bf813ab77d1b89180c1d4977a6044195c61b26094fd8",
            "target_hash": "65756e8ee178cfa13774408f31eabb354c71d2029c74876029c1f24d71c34d15",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 192,
            "doc": {
                "candidates": [
                    "(A) A pair of glasses",
                    "(B) A cup of coffee",
                    "(C) A glass of milk",
                    "(D) A pair of gloves"
                ],
                "answer": "(A) A pair of glasses",
                "time_reference": "52:40-52:31",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What did the man interviewed after Sergey Sarancha hold in his hand?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(A) A pair of glasses",
            "arguments": [
                "What did the man interviewed after Sergey Sarancha hold in his hand?\nA. (A) A pair of glasses\nB. (B) A cup of coffee\nC. (C) A glass of milk\nD. (D) A pair of gloves\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2aa0>"
                    ]
                },
                192,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4b71fe888d1612227f8cbf796a566cc69b65a4cc2a04395437da458f5ab2e117",
            "target_hash": "51fea79d7282348557a66b867f025739f66fa370139f64e948c9ecc262cd5016",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 193,
            "doc": {
                "candidates": [
                    "(A) Five-pointed star",
                    "(B) Flower",
                    "(C) Pochacco",
                    "(D) Stlch"
                ],
                "answer": "(A) Five-pointed star",
                "time_reference": "52:52-52:54",
                "question_type": [
                    "entity recognition"
                ],
                "question": "What is the tattoo pattern on the arm of the woman who is using a telescope wearing a bracelet?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(A) Five-pointed star",
            "arguments": [
                "What is the tattoo pattern on the arm of the woman who is using a telescope wearing a bracelet?\nA. (A) Five-pointed star\nB. (B) Flower\nC. (C) Pochacco\nD. (D) Stlch\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd3a60>"
                    ]
                },
                193,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "393eb59310ef5cdb958040c4ede80a2a5400518d3467f55273cbcbdde0a62c0c",
            "target_hash": "b15b56ac146de765c85864751a1ffa06347154b79e7de306d8d712eb893d03e3",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 194,
            "doc": {
                "candidates": [
                    "(A) No smoking",
                    "(B) Safety first",
                    "(C) Don't be late",
                    "(D) Beware of danger"
                ],
                "answer": "(A) No smoking",
                "time_reference": "54:18-54:27",
                "question_type": [
                    "key information retrieval",
                    "entity recognition"
                ],
                "question": "What was written on the building behind Arve Nordeide when he came from a distance?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(A) No smoking",
            "arguments": [
                "What was written on the building behind Arve Nordeide when he came from a distance?\nA. (A) No smoking\nB. (B) Safety first\nC. (C) Don't be late\nD. (D) Beware of danger\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f46e5fd2e60>"
                    ]
                },
                194,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ae15b7511f29dfe4b07e7e89160501764fc828b7f6b140ad066a1a23ebd89d62",
            "target_hash": "d9da720eb1d622b51b76d6288bea2b8dd9469de791f87c36fa56520e4d2a29ed",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 195,
            "doc": {
                "candidates": [
                    "(A) Signature ceremony",
                    "(B) Crew shooting",
                    "(C) Press conference",
                    "(D) Examination room"
                ],
                "answer": "(C) Press conference",
                "time_reference": "55:41-55:43",
                "question_type": [
                    "reasoning",
                    "event understanding"
                ],
                "question": "What is the occasion right after the last interview scene with Mohamed Abshir Waldo?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(C) Press conference",
            "arguments": [
                "What is the occasion right after the last interview scene with Mohamed Abshir Waldo?\nA. (A) Signature ceremony\nB. (B) Crew shooting\nC. (C) Press conference\nD. (D) Examination room\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac0457b0>"
                    ]
                },
                195,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cc64bc30f20eea7891f19ac69bd1a089aa2ef2337b0f6b23ec35de9bfd75bd06",
            "target_hash": "931db6de9b4e280a0e8cb359c6e6f663dd8390c581a335e06bba1a36fc0d76fb",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 196,
            "doc": {
                "candidates": [
                    "(A) 0",
                    "(B) 3",
                    "(C) 1",
                    "(D) 2"
                ],
                "answer": "(D) 2",
                "time_reference": "09:10-52:16",
                "question_type": [
                    "event understanding"
                ],
                "question": "How many times did Sergey Sarancha's interview segments appear?",
                "type": "documentary",
                "video": "vaL_vSdZKZo"
            },
            "target": "(D) 2",
            "arguments": [
                "How many times did Sergey Sarancha's interview segments appear?\nA. (A) 0\nB. (B) 3\nC. (C) 1\nD. (D) 2\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f44ac046710>"
                    ]
                },
                196,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "19b2f18fa751788bb5d5e511380fb2180714cd76fcd974343438cc63b82d061f",
            "target_hash": "610f912643698b744baa44b5b7ac325d3350523ffbbbb3b9209c49f827b3123f",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        }
    ],
    "time": "0630_1705"
}